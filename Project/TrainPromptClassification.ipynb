{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing finetune_mixtral_with_fulldata2_v6.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile finetune_mixtral_with_fulldata2_v6.py\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "import shutil\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DistributedSampler,Dataset, DataLoader\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import MixedPrecision\n",
    "from transformers.models.mixtral.modeling_mixtral import MixtralDecoderLayer\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, default_data_collator, get_cosine_schedule_with_warmup, AutoConfig, BitsAndBytesConfig\n",
    "import gc\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import subprocess\n",
    "import utils\n",
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "from torch.nn import functional as F\n",
    "\n",
    "prompt_part1 = \\\n",
    "f'''You are a website spam expert. You are given information about a webpage to judge whether or not it is spam. 0 means nonspam and 1 means spam. Give your prediction after the <ANS>: tag.\n",
    "    Url: {{Url}}\n",
    "    UrlTitle: {{UrlTitle}}\n",
    "    UrlSnippet: {{UrlSnippet}} \n",
    "    Site Content: {{FullBody}}\n",
    "'''\n",
    "\n",
    "prompt_part2 = \\\n",
    "f'''\n",
    "What is your prediction <ANS>:{{Label}}'''\n",
    "\n",
    "IGNORE_INDEX = -100  # The default setting in CrossEntropyLoss\n",
    "# MAX_LENGTH_EVAL = 1024\n",
    "\n",
    "class TrainingDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_seq_length):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        def apply_prompt1_template(sample):\n",
    "            return prompt_part1.format(Url=sample['Url'],\n",
    "                                        UrlTitle=sample['UrlTitle'],\n",
    "                                        UrlSnippet=sample['UrlSnippet'],\n",
    "                                        FullBody=sample['FullBody'])\n",
    "        \n",
    "        def apply_prompt2_template(sample):\n",
    "            return prompt_part2.format(Label=sample['Label'])\n",
    "        \n",
    "        \n",
    "        row = self.df.iloc[idx]\n",
    "        text_part_1 = apply_prompt1_template(row[[\"Url\",\"UrlTitle\",\"UrlSnippet\",\"FullBody\"]])\n",
    "        text_part_2 = apply_prompt2_template(row[[\"Label\"]])\n",
    "\n",
    "        res = self.tokenizer(f\"{self.tokenizer.bos_token} {text_part_1}\", text_part_2, add_special_tokens=False, max_length=self.max_seq_length, padding='max_length', truncation='only_first')\n",
    "        \n",
    "        labels = torch.tensor(copy.deepcopy(res['input_ids']), dtype=torch.int64)\n",
    "        actual_token_len = sum(res['attention_mask'])\n",
    "\n",
    "        labels[:actual_token_len-1] = IGNORE_INDEX  \n",
    "        labels[actual_token_len:] = IGNORE_INDEX\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(res['input_ids']),\n",
    "            'attention_mask': torch.tensor(res['attention_mask']),\n",
    "            'labels':labels\n",
    "        }\n",
    "\n",
    "def setup(args):\n",
    "    # setup distributed environment\n",
    "    world_size = int(os.environ[\"WORLD_SIZE\"])  # 参与训练的总进程数，即GPU数量。在分布式训练中，每个GPU对应一个进程。因此，world_size是你在分布式训练中使用的GPU数量\n",
    "    rank = int(os.environ[\"RANK\"])      # rank是每个进程的唯一标识符，用于区分不同的进程。在分布式训练中，每个进程都有一个独特的rank，从0到world_size-1\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])  # local_rank:是在单个节点内部的进程的本地标识符。如果你在多个节点上运行分布式训练，每个节点都有自己的local_rank。\n",
    "    # 比如，你在2台服务器上运行分布式训练，每台服务器有4个GPU。那么总的world_size是8，进程的rank分别是[0,1,2,3,4,5,6,7]。在每台服务器上local_rank分别是[0,1,2,3]\n",
    "    print(f\"World size: {world_size}, rank: {rank}, local rank: {local_rank}\")\n",
    "\n",
    "    timeout = timedelta(hours=5) # 时间间隔对象，表示在初始化分布式进程组时等待的最大时间。这里表示如果在5小时内无法成功初始化分布式进程组，将引发超时错误\n",
    "    dist.init_process_group(\"nccl\", timeout=timeout, rank=rank, world_size=world_size) # 初始化分布式进程组，nccl表示用用NVIDIA Collective Communications Library作为后端，rank表示当前进程的标识符，world_size表示总进程数\n",
    "    assert torch.distributed.is_initialized()\n",
    "\n",
    "    torch.cuda.set_device(local_rank)  # 设置当前进程使用的GPU设备，local_rank表示当前节点内部的进程的本地标识符\n",
    "    torch.cuda.empty_cache()    # 用于清空GPU缓存，以释放内存\n",
    "\n",
    "    if is_master(rank): # 只有当前是主进程时才会执行打印参数的操作\n",
    "        print(args)\n",
    "    \n",
    "    # setup tokenizer and dataloader\n",
    "    \n",
    "    # if args.model == 'llama2':\n",
    "    #     tokenizer = LlamaTokenizer.from_pretrained(args.load_from)\n",
    "    # elif args.model == 'mistral':\n",
    "    #     tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "    # elif args.model == 'turing':\n",
    "    #     # tokenizer = TNLGv4Tokenizer()\n",
    "    #     tokenizer = GPT2TokenizerFast.from_pretrained('Xenova/gpt-4')\n",
    "    # elif args.model == 'mixtral':\n",
    "    #     tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\")\n",
    "    # else:\n",
    "    #     print(f\"model {args.model} not supported\")\n",
    "    #     raise NotImplementedError\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.load_from,add_bos_token=True,trust_remote_code=True)\n",
    "    tokenizer.padding_side = 'left'\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # setup model and fsdp\n",
    "    if args.model == 'llama2':\n",
    "        model = LlamaForCausalLM.from_pretrained(args.load_from, load_in_8bit=False, device_map=None, torch_dtype=torch.bfloat16, use_cache=True)\n",
    "        if args.neftune_alpha is not None:\n",
    "            print('NEFTUNE enabled')\n",
    "            # Save the old forward function as a class attribute\n",
    "            torch.nn.Embedding.old_forward = model.model.embed_tokens.forward\n",
    "\n",
    "            # Define the new forward function\n",
    "            def new_forward(self, x):\n",
    "                # Call the old forward function and get its output\n",
    "                #print('neftune forward')\n",
    "                out = self.old_forward(x)\n",
    "                dims = torch.tensor(out.size(1) * out.size(2))\n",
    "                mag_norm = args.neftune_alpha / torch.sqrt(dims)\n",
    "                return out + torch.zeros_like(out).uniform_(-mag_norm, mag_norm)\n",
    "\n",
    "            # Replace the forward function of the embedding object with the new one\n",
    "            model.model.embed_tokens.forward = new_forward.__get__(model.model.embed_tokens, torch.nn.Embedding)\n",
    "    elif args.model == \"mistral\":\n",
    "        model = MistralForCausalLM.from_pretrained(args.load_from, load_in_8bit=False, device_map=None, torch_dtype=torch.float16, use_cache=True)\n",
    "    elif args.model == \"mixtral\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(args.load_from,\n",
    "                                                    load_in_8bit=False, # 让模型不要以8位精度加载，使用默认精度加载权重\n",
    "                                                    device_map=None,    # 没有自定义的设备映射，此时如果你在多个GPU上运行，模型的参数将均匀分配到所有可用的GPU上，适用于参数量大的模型\n",
    "                                                    torch_dtype=torch.bfloat16,  # 用16位浮点数作为权重数据类型\n",
    "                                                    use_cache=True  # 模型将使用缓存来加速计算\n",
    "                                                    )\n",
    "        # model_state_dict = torch.load(args.model_path)\n",
    "        # model.load_state_dict(model_state_dict)\n",
    "    else:\n",
    "        print(f\"model {args.model} not supported\")\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    if is_master(rank):\n",
    "        print(f\"model config: {model.config}\")\n",
    "        print(model)\n",
    "        \n",
    "    model.config.pad_token_id = model.config.bos_token_id  # 再做实验可以尝试取消这个\n",
    "    \n",
    "    if args.model == 'turing': #TODO: Verify this is not causing loss degradation\n",
    "        model.config.bos_token_id = tokenizer.bos_token_id\n",
    "        model.config.eos_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    model.to(dtype=torch.bfloat16)\n",
    "\n",
    "    #utils.apply_fsdp_checkpointing(model) if args.model == 'llama2' else model.gradient_checkpointing_enable() # for turing\n",
    "    model.gradient_checkpointing_enable()\n",
    "    #print(f\"Gradient Checkpointing: {model.is_gradient_checkpointing}\")\n",
    "\n",
    "    model = FSDP(\n",
    "        model,\n",
    "        auto_wrap_policy=utils.get_model_wrapper(),\n",
    "        #mixed_precision=mixed_precision_policy,\n",
    "        mixed_precision=None,\n",
    "        sharding_strategy=utils.fsdp_config.sharding_strategy,\n",
    "        device_id=torch.cuda.current_device(),\n",
    "        limit_all_gathers=True,\n",
    "        sync_module_states=False,\n",
    "        param_init_fn=None\n",
    "    )\n",
    "\n",
    "    # utils.apply_fsdp_checkpointing(model) if args.model == 'llama2' else model.gradient_checkpointing_enable() # for turing\n",
    "    \n",
    "    # setup optimizer\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=args.lr,\n",
    "        weight_decay=0.0,\n",
    "    )\n",
    "    \n",
    "    # df = pd.read_csv(\"/cosmos/local/IndexQuality/FinetuneLLM/TrainingData/O1_A3_crowd_training_data_1.tsv\",sep=\"\\t\")\n",
    "    df = pd.read_csv(args.training_data_from, sep='\\t', usecols=args.columns)\n",
    "    df = df[df['Label'].notna()]\n",
    "    print(f'training dataset length: {len(df)}')\n",
    "    print(f'training data columns: {df.columns}')\n",
    "    print(f'training data pos/neg counts: {df.Label.value_counts()}')\n",
    "    \n",
    "    train_ds =  TrainingDataset(df, tokenizer, args.max_seq_length)\n",
    "    \n",
    "    train_sampler = DistributedSampler(\n",
    "        train_ds,\n",
    "        rank=rank,\n",
    "        num_replicas=world_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.num_workers,  # 4\n",
    "        pin_memory=True,\n",
    "        sampler=train_sampler,\n",
    "        drop_last=True,\n",
    "        collate_fn=default_data_collator,\n",
    "    )\n",
    "\n",
    "    total_iteration = args.num_epochs * (len(train_dataloader)/(args.batch_size*args.gpu_counts))\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=len(train_dataloader) * args.warmup, # args.warmup=0.1\n",
    "        num_training_steps=len(train_dataloader) * args.num_epochs\n",
    "        # num_warmup_steps=total_iteration*0.1,  # warmup阶段可以帮助模型在训练的早期阶段更稳定地开始学习。这通常设置为总迭代次数的一个较小的比例，比如总迭代次数的10%。\n",
    "        # num_training_steps=total_iteration  # 总迭代次数=epoch*Iterations per epoch = epoch*(total_data/(batch_size*gpu_counts))。\n",
    "    )\n",
    "\n",
    "    return model, train_dataloader, optimizer, scheduler, local_rank, rank, world_size, tokenizer\n",
    "\n",
    "\n",
    "def is_master(rank):\n",
    "    # In a multi-node setup, the master process is rank 0\n",
    "    return rank == 0\n",
    "\n",
    "class RunningMean(object):\n",
    "    def __init__(self, local_rank, N=100):\n",
    "        self.N = N\n",
    "        self.local_rank = local_rank\n",
    "        self.data = []\n",
    "\n",
    "    def add(self, x):\n",
    "        self.data.append(x)\n",
    "\n",
    "    def mean_all_rank(self):\n",
    "        d = self.data[-self.N:]\n",
    "        if len(d) > 0:\n",
    "            m = sum(d) / len(d)\n",
    "        else:\n",
    "            m = 0\n",
    "        t = torch.tensor([m]).to(self.local_rank)\n",
    "        all_t = torch.zeros(dist.get_world_size(), dtype=t.dtype).to(self.local_rank)\n",
    "        dist.all_gather_into_tensor(all_t, t)\n",
    "        return all_t.mean().cpu().item()\n",
    "            \n",
    "def train(args, model, train_dataloader, optimizer, scheduler, local_rank, rank, world_size, tokenizer):\n",
    "    rm = RunningMean(local_rank)\n",
    "    global_step = 0\n",
    "\n",
    "    if global_step == 0 and is_master(rank):\n",
    "        # Initialize TensorBoard writer only on the master process\n",
    "        print('init tensorboard,', args.experiment_name)\n",
    "        writer = SummaryWriter(args.log_dir)\n",
    "        # with open(os.path.join(args.output_dir, \"log.csv\"), 'w') as f_log:\n",
    "        #     f_log.write('global_step, total_loss\\n')\n",
    "\n",
    "    gradient_accumulation_steps = args.gradient_accumulation_steps  # bs = 10, so total_bs is 250\n",
    "    for epoch in range(args.num_epochs):\n",
    "        for step, data in tqdm(enumerate(train_dataloader), total=len(train_dataloader), disable=not is_master(rank), desc=f'Epoch {epoch}/{args.num_epochs}'):\n",
    "            model.train()\n",
    "            \n",
    "            loss = model(**data).loss\n",
    "            # accululating gradients over steps\n",
    "            if gradient_accumulation_steps > 1:\n",
    "                loss = loss / gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "            # print(f'--->>> step: {step}')\n",
    "            if step % gradient_accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "                \n",
    "            rm.add(loss.item())\n",
    "            mean_loss = rm.mean_all_rank()\n",
    "            if is_master(rank):\n",
    "                print(f'loss: {mean_loss}')\n",
    "                writer.add_scalar('Loss/train', mean_loss, global_step)      \n",
    "                # with open(os.path.join(args.output_dir, \"log.csv\"), \"a\") as f_log:\n",
    "                #     f_log.write(str(global_step)+\",\"+str(mean_loss)+\"\\n\")\n",
    "            \n",
    "            if(global_step%(args.save_checkpoint_steps*gradient_accumulation_steps)==0 and global_step>0):\n",
    "                dir_name = 'model_'+str(global_step)\n",
    "                checkpoint_dir = os.path.join(args.output_dir, dir_name)\n",
    "                if not os.path.exists(checkpoint_dir):\n",
    "                    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "                print(\"saving checkpoint...\")\n",
    "                utils.save_model_checkpoint(model, checkpoint_dir, rank)\n",
    "                # if is_master(rank):\n",
    "                    # os.popen(f'cp {args.load_from}/*.json {checkpoint_dir}')\n",
    "                    # os.popen(f'cp {args.load_from}/token* {checkpoint_dir}')\n",
    "                #     os.popen(f'cp {args.load_from}/config.json {checkpoint_dir}')\n",
    "                # print(\"checkpoint saved\")\n",
    "                \n",
    "                # if is_master(rank):\n",
    "                #     print(f\"rank: {rank} running eval...\")\n",
    "                    # Execute evaluation only on the master process\n",
    "                    # if args.reversed:\n",
    "                    #     eval_command = f\"CUDA_VISIBLE_DEVICES=14,15 python -m torch.distributed.run --master_port 29501 --nnodes 1 --nproc_per_node 2 summary_eval_batch.py --reversed --max_new_tokens {args.max_new_tokens} --output_dir {checkpoint_dir} --eval_file {args.eval_file} --num_eval {args.num_eval} --model {args.model} --load_from {checkpoint_dir} --batch_size 10\"\n",
    "                    # else:\n",
    "                        # eval_command = f\"CUDA_VISIBLE_DEVICES=14,15 python -m torch.distributed.run --master_port 29501 --nnodes 1 --nproc_per_node 2 summary_eval_batch.py --max_new_tokens {args.max_new_tokens} --output_dir {checkpoint_dir} --eval_file {args.eval_file} --num_eval {args.num_eval} --model {args.model} --load_from {checkpoint_dir} --batch_size 10\"\n",
    "                    # eval_command = f\"CUDA_VISIBLE_DEVICES=15,16 python -m torch.distributed.run --master_port 29501 --nnodes 1 --nproc_per_node 2 inf_test.py --batch_size {args.batch_size} --models_dir {checkpoint_dir} --eval_data_from {args.eval_data_from}\"\n",
    "                    # print(eval_command)\n",
    "                    # new_process = multiprocessing.Process(target=eval_process, args=(eval_command, args, global_step, writer), name=\"New Evaluation Process\")\n",
    "                    # Start the new process asynchronously\n",
    "                    # new_process.start()\n",
    "                \n",
    "            dist.barrier()\n",
    "            global_step += 1\n",
    "        \n",
    "    if is_master(rank):\n",
    "        writer.flush()\n",
    "        writer.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # parser.add_argument('--load_from', type=str, default='/data/local/IndexQuality/FinetuneLLM/Phi-3-medium')\n",
    "    parser.add_argument('--load_from', type=str, default='/cosmos/local/IndexQuality/FinetuneLLM/Mixtral-8x7B-Instruct-v0.1/')\n",
    "    parser.add_argument('--model_path', type=str, default='/cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/Mixtral_New_ym_bs_10_test/model_1900/pytorch_model.bin')\n",
    "    parser.add_argument('--model', type=str, default='mixtral')\n",
    "    parser.add_argument('--num_workers', type=int, default=4)\n",
    "    parser.add_argument('--warmup', type=float, default=0.1)\n",
    "    parser.add_argument('--training_data_from', type=str, default=\"/cosmos/local/IndexQuality/FinetuneLLM/TrainingData/O1_A3_crowd_llm_training_data_no_eval_dataset_overlap.tsv\")\n",
    "    parser.add_argument('--columns', '-c', nargs='+', required=True, help=\"Names of columns to read\")\n",
    "    # parser.add_argument('--eval_data_from', type=str, default=\"/cosmos/local/IndexQuality/FinetuneLLM/EvaluationSets/scrapekr1.2_spamllm2.4.parquet\")\n",
    "    # parser.add_argument('--eval_data_from', type=str, default=\"/cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/Mixtral_New_ym_bs_10_test/scrapekr1.2_spamllm2.4_sample_20.parquet\")\n",
    "    parser.add_argument('--batch_size', type=int, default=10) # 8->10 \n",
    "    parser.add_argument('--gradient_accumulation_steps', type=int, default=2)\n",
    "    parser.add_argument('--num_epochs', type=int, default=2)\n",
    "    parser.add_argument('--max_seq_length', type=int, default=1024)\n",
    "    # parser.add_argument('--eval_steps', type=int, default=200)  # 平常隔200评估一次，如果效果比之前好，直接保存；否则按save_checkpoint_steps保存\n",
    "    parser.add_argument('--save_checkpoint_steps', type=int, default=500)\n",
    "    parser.add_argument('--lr', type=float, default=1e-5)\n",
    "    parser.add_argument('--gpu_counts', type=float, default=16) # warmup=0.1\n",
    "    parser.add_argument('--output_dir', type=str, required=True)\n",
    "    parser.add_argument('--experiment_name', type=str, required=True)\n",
    "    parser.add_argument('--log_dir', type=str, required=True)\n",
    "    args = parser.parse_args()\n",
    "    train(args, *setup(args))\n",
    "    \n",
    "# singularity Command\n",
    "# pip install transformers[torch]==4.38.1 datasets scikit-learn dataclasses lightgbm matplotlib mlflow tensorboard && cd /cosmos/local/IndexQuality/ContentModels/DataAugmentation/data/CBSpam_v3/Code/FinetuneLLM-US/new/ && CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 torchrun --nnodes 1 --nproc_per_node 16 train_script_optimized_test.py --load_from /cosmos/local/IndexQuality/FinetuneLLM/Mixtral-8x7B-Instruct-v0.1/ --model mixtral --num_workers 4 --training_data_from /cosmos/local/IndexQuality/FinetuneLLM/TrainingData/O1_A3_crowd_llm_training_data_no_eval_dataset_overlap.tsv --batch_size 10 --gradient_accumulation_steps 2 --num_epochs 2 --save_checkpoint_steps 200 --lr 1e-5 --output_dir /cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/Mixtral_New_ym_bs_10_test --experiment_name MixtralTrain_ym --disable_tensorboard False --log_dir /cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/Mixtral_New_ym_bs_10_test/log\n",
    "# current running: Mixtral_again: https://ml.azure.com/runs/Mixtral_again?wsid=/subscriptions/7972af26-e54d-410e-a755-20e582a46de0/resourceGroups/singularity-webdata/providers/Microsoft.MachineLearningServices/workspaces/singularity-webdata-ws01-eastus2&flight=1ptraining&tid=72f988bf-86f1-41af-91ab-2d7cd011db47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "[2024-07-12 01:31:23,011] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-07-12 01:31:23,094] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-07-12 01:31:23,163] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:118: UserWarning: onnxruntime training package info: package_name: onnxruntime-training\n",
      "  warnings.warn(\"onnxruntime training package info: package_name: %s\" % package_name)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:119: UserWarning: onnxruntime training package info: __version__: 1.17.1\n",
      "  warnings.warn(\"onnxruntime training package info: __version__: %s\" % version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:120: UserWarning: onnxruntime training package info: cuda_version: 12.2\n",
      "  warnings.warn(\"onnxruntime training package info: cuda_version: %s\" % cuda_version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:121: UserWarning: onnxruntime build info: cudart_version: 12020\n",
      "  warnings.warn(\"onnxruntime build info: cudart_version: %s\" % cudart_version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:129: UserWarning: WARNING: failed to find cudart version that matches onnxruntime build info\n",
      "  warnings.warn(\"WARNING: failed to find cudart version that matches onnxruntime build info\")\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:130: UserWarning: WARNING: found cudart versions: [12010]\n",
      "  warnings.warn(\"WARNING: found cudart versions: %s\" % local_cudart_versions)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:118: UserWarning: onnxruntime training package info: package_name: onnxruntime-training\n",
      "  warnings.warn(\"onnxruntime training package info: package_name: %s\" % package_name)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:119: UserWarning: onnxruntime training package info: __version__: 1.17.1\n",
      "  warnings.warn(\"onnxruntime training package info: __version__: %s\" % version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:120: UserWarning: onnxruntime training package info: cuda_version: 12.2\n",
      "  warnings.warn(\"onnxruntime training package info: cuda_version: %s\" % cuda_version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:121: UserWarning: onnxruntime build info: cudart_version: 12020\n",
      "  warnings.warn(\"onnxruntime build info: cudart_version: %s\" % cudart_version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:129: UserWarning: WARNING: failed to find cudart version that matches onnxruntime build info\n",
      "  warnings.warn(\"WARNING: failed to find cudart version that matches onnxruntime build info\")\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:130: UserWarning: WARNING: found cudart versions: [12010]\n",
      "  warnings.warn(\"WARNING: found cudart versions: %s\" % local_cudart_versions)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:118: UserWarning: onnxruntime training package info: package_name: onnxruntime-training\n",
      "  warnings.warn(\"onnxruntime training package info: package_name: %s\" % package_name)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:119: UserWarning: onnxruntime training package info: __version__: 1.17.1\n",
      "  warnings.warn(\"onnxruntime training package info: __version__: %s\" % version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:120: UserWarning: onnxruntime training package info: cuda_version: 12.2\n",
      "  warnings.warn(\"onnxruntime training package info: cuda_version: %s\" % cuda_version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:121: UserWarning: onnxruntime build info: cudart_version: 12020\n",
      "  warnings.warn(\"onnxruntime build info: cudart_version: %s\" % cudart_version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:129: UserWarning: WARNING: failed to find cudart version that matches onnxruntime build info\n",
      "  warnings.warn(\"WARNING: failed to find cudart version that matches onnxruntime build info\")\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:130: UserWarning: WARNING: found cudart versions: [12010]\n",
      "  warnings.warn(\"WARNING: found cudart versions: %s\" % local_cudart_versions)\n",
      "World size: 3, rank: 2, local rank: 2\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "World size: 3, rank: 0, local rank: 0\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "Namespace(load_from='/cosmos/local/IndexQuality/FinetuneLLM/Mixtral-8x7B-Instruct-v0.1/', model_path='/cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/Mixtral_New_ym_bs_10_test/model_1900/pytorch_model.bin', model='mixtral', num_workers=4, warmup=0.1, training_data_from='/cosmos/local/IndexQuality/FinetuneLLM/TrainingData/zifan_pusudo_gt_unique_hosts_pos_plus_neg_for_finetune.csv', columns=['Url', 'UrlTitle', 'UrlSnippet', 'FullBody', 'Label'], batch_size=1, gradient_accumulation_steps=2, num_epochs=2, max_seq_length=1024, save_checkpoint_steps=100, lr=1e-05, gpu_counts=16.0, output_dir='/cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/Mixtral_New_ym_bs_10_test_v2', experiment_name='mixtral_e5_v2', log_dir='/cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/Mixtral_New_ym_bs_10_test_v2/logs')\n",
      "World size: 3, rank: 1, local rank: 1\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "Loading checkpoint shards: 100%|████████████████| 19/19 [03:02<00:00,  9.63s/it]\n",
      "Loading checkpoint shards: 100%|████████████████| 19/19 [03:02<00:00,  9.62s/it]\n",
      "Loading checkpoint shards: 100%|████████████████| 19/19 [03:02<00:00,  9.62s/it]\n",
      "Model auto wrap policy:  functools.partial(<function transformer_auto_wrap_policy at 0x7fc7a18a4430>, transformer_layer_cls={<class 'torch.nn.modules.sparse.Embedding'>, <class 'transformers.models.mixtral.modeling_mixtral.MixtralDecoderLayer'>})\n",
      "Model auto wrap policy:  functools.partial(<function transformer_auto_wrap_policy at 0x7f729b89c430>, transformer_layer_cls={<class 'torch.nn.modules.sparse.Embedding'>, <class 'transformers.models.mixtral.modeling_mixtral.MixtralDecoderLayer'>})\n",
      "model config: MixtralConfig {\n",
      "  \"_name_or_path\": \"/cosmos/local/IndexQuality/FinetuneLLM/Mixtral-8x7B-Instruct-v0.1/\",\n",
      "  \"architectures\": [\n",
      "    \"MixtralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mixtral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_experts_per_tok\": 2,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"num_local_experts\": 8,\n",
      "  \"output_router_logits\": false,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"router_aux_loss_coef\": 0.02,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "MixtralForCausalLM(\n",
      "  (model): MixtralModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MixtralDecoderLayer(\n",
      "        (self_attn): MixtralSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MixtralRotaryEmbedding()\n",
      "        )\n",
      "        (block_sparse_moe): MixtralSparseMoeBlock(\n",
      "          (gate): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (experts): ModuleList(\n",
      "            (0-7): 8 x MixtralBlockSparseTop2MLP(\n",
      "              (w1): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "              (w2): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "              (w3): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (input_layernorm): MixtralRMSNorm()\n",
      "        (post_attention_layernorm): MixtralRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): MixtralRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n",
      "Model auto wrap policy:  functools.partial(<function transformer_auto_wrap_policy at 0x7f19478c0430>, transformer_layer_cls={<class 'torch.nn.modules.sparse.Embedding'>, <class 'transformers.models.mixtral.modeling_mixtral.MixtralDecoderLayer'>})\n",
      "training dataset length: 273952\n",
      "training data columns: Index(['Url', 'UrlTitle', 'UrlSnippet', 'FullBody', 'Label'], dtype='object')\n",
      "training data pos/neg counts: Label\n",
      "0.0    250000\n",
      "1.0     23952\n",
      "Name: count, dtype: int64\n",
      "training dataset length: 273952\n",
      "training data columns: Index(['Url', 'UrlTitle', 'UrlSnippet', 'FullBody', 'Label'], dtype='object')\n",
      "training data pos/neg counts: Label\n",
      "0.0    250000\n",
      "1.0     23952\n",
      "Name: count, dtype: int64\n",
      "training dataset length: 273952\n",
      "training data columns: Index(['Url', 'UrlTitle', 'UrlSnippet', 'FullBody', 'Label'], dtype='object')\n",
      "training data pos/neg counts: Label\n",
      "0.0    250000\n",
      "1.0     23952\n",
      "Name: count, dtype: int64\n",
      "init tensorboard, mixtral_e5_v2\n",
      "Epoch 0/2:   0%|                                      | 0/91318 [00:00<?, ?it/s]NCCL version 2.19.4+cuda12.1\n",
      "\n",
      "node-0:2678107:2681241 [0] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:2678107:2681241 [0] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_0\n",
      "\n",
      "node-0:2678107:2681241 [0] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:2678107:2681241 [0] transport/net_ib.cc:193 NCCL WARN NET/IB : Unable to open device mlx5_0\n",
      "\n",
      "node-0:2678109:2681243 [2] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:2678109:2681243 [2] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_0\n",
      "\n",
      "node-0:2678109:2681243 [2] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:2678109:2681243 [2] transport/net_ib.cc:193 NCCL WARN NET/IB : Unable to open device mlx5_0\n",
      "\n",
      "node-0:2678108:2681245 [1] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:2678108:2681245 [1] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_0\n",
      "\n",
      "node-0:2678108:2681245 [1] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:2678108:2681245 [1] transport/net_ib.cc:193 NCCL WARN NET/IB : Unable to open device mlx5_0\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "Epoch 0/2:   0%|                                      | 0/91318 [00:35<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/cosmos/local/IndexQuality/ContentModels/DataAugmentation/data/CBSpam_v3/Code/FinetuneLLM-US/new/finetune_mixtral_with_fulldata2_v2.py\", line 345, in <module>\n",
      "    train(args, *setup(args))\n",
      "  File \"/cosmos/local/IndexQuality/ContentModels/DataAugmentation/data/CBSpam_v3/Code/FinetuneLLM-US/new/finetune_mixtral_with_fulldata2_v2.py\", line 275, in train\n",
      "    optimizer.step()\n",
      "  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/optim/lr_scheduler.py\", line 75, in wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 385, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 76, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/optim/adamw.py\", line 176, in step\n",
      "    has_complex = self._init_group(\n",
      "  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/optim/adamw.py\", line 127, in _init_group\n",
      "    state[\"exp_avg_sq\"] = torch.zeros_like(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 924.00 MiB. GPU 1 has a total capacity of 79.15 GiB of which 355.31 MiB is free. Process 1087405 has 78.79 GiB memory in use. Of the allocated memory 74.15 GiB is allocated by PyTorch, and 3.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Traceback (most recent call last):\n",
      "  File \"/cosmos/local/IndexQuality/ContentModels/DataAugmentation/data/CBSpam_v3/Code/FinetuneLLM-US/new/finetune_mixtral_with_fulldata2_v2.py\", line 345, in <module>\n",
      "    train(args, *setup(args))\n",
      "  File \"/cosmos/local/IndexQuality/ContentModels/DataAugmentation/data/CBSpam_v3/Code/FinetuneLLM-US/new/finetune_mixtral_with_fulldata2_v2.py\", line 275, in train\n",
      "    optimizer.step()\n",
      "  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/optim/lr_scheduler.py\", line 75, in wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 385, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 76, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/optim/adamw.py\", line 176, in step\n",
      "    has_complex = self._init_group(\n",
      "  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/optim/adamw.py\", line 127, in _init_group\n",
      "    state[\"exp_avg_sq\"] = torch.zeros_like(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 924.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 355.31 MiB is free. Process 1087404 has 78.79 GiB memory in use. Of the allocated memory 74.15 GiB is allocated by PyTorch, and 3.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Traceback (most recent call last):\n",
      "  File \"/cosmos/local/IndexQuality/ContentModels/DataAugmentation/data/CBSpam_v3/Code/FinetuneLLM-US/new/finetune_mixtral_with_fulldata2_v2.py\", line 345, in <module>\n",
      "    train(args, *setup(args))\n",
      "  File \"/cosmos/local/IndexQuality/ContentModels/DataAugmentation/data/CBSpam_v3/Code/FinetuneLLM-US/new/finetune_mixtral_with_fulldata2_v2.py\", line 275, in train\n",
      "    optimizer.step()\n",
      "  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/optim/lr_scheduler.py\", line 75, in wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 385, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 76, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/optim/adamw.py\", line 176, in step\n",
      "    has_complex = self._init_group(\n",
      "  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/optim/adamw.py\", line 127, in _init_group\n",
      "    state[\"exp_avg_sq\"] = torch.zeros_like(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 924.00 MiB. GPU 2 has a total capacity of 79.15 GiB of which 405.31 MiB is free. Process 1087406 has 78.74 GiB memory in use. Of the allocated memory 74.15 GiB is allocated by PyTorch, and 3.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "[2024-07-12 01:39:25,409] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 2678107) of binary: /opt/conda/envs/ptca/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/ptca/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/opt/conda/envs/ptca/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/distributed/run.py\", line 816, in <module>\n",
      "    main()\n",
      "  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/distributed/run.py\", line 812, in main\n",
      "    run(args)\n",
      "  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/distributed/run.py\", line 803, in run\n",
      "    elastic_launch(\n",
      "  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 135, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 268, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "finetune_mixtral_with_fulldata2_v2.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "[1]:\n",
      "  time      : 2024-07-12_01:39:25\n",
      "  host      : node-0\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : 1 (pid: 2678108)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "[2]:\n",
      "  time      : 2024-07-12_01:39:25\n",
      "  host      : node-0\n",
      "  rank      : 2 (local_rank: 2)\n",
      "  exitcode  : 1 (pid: 2678109)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2024-07-12_01:39:25\n",
      "  host      : node-0\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 2678107)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=\"0,1,2\"\n",
    "!NCCL_DEBUG=WARN python -m torch.distributed.run  \\\n",
    "--master_port 29501 --nnodes 1 --nproc_per_node 3 finetune_mixtral_with_fulldata2_v3.py \\\n",
    "--load_from /cosmos/local/IndexQuality/FinetuneLLM/Mixtral-8x7B-Instruct-v0.1/ \\\n",
    "--model mixtral --num_workers 4 \\\n",
    "--training_data_from /cosmos/local/IndexQuality/FinetuneLLM/TrainingData/zifan_pusudo_gt_unique_hosts_pos_plus_neg_for_finetune.csv \\\n",
    "--columns Url UrlTitle UrlSnippet FullBody Label \\\n",
    "--batch_size 10 \\\n",
    "--gradient_accumulation_steps 2 \\\n",
    "--num_epochs 2 \\\n",
    "--max_seq_length 1024 \\\n",
    "--save_checkpoint_steps 100 \\\n",
    "--lr 1e-5 \\\n",
    "--gpu_counts 16 \\\n",
    "--output_dir /cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/Mixtral_New_ym_bs_10_test_v2 \\\n",
    "--experiment_name mixtral_e5_v2 \\\n",
    "--log_dir /cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/Mixtral_New_ym_bs_10_test_v2/logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-07-03 03:32:56,855] torch.distributed.run: [WARNING] \n",
      "[2024-07-03 03:32:56,855] torch.distributed.run: [WARNING] *****************************************\n",
      "[2024-07-03 03:32:56,855] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "[2024-07-03 03:32:56,855] torch.distributed.run: [WARNING] *****************************************\n",
      "[2024-07-03 03:33:00,096] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-07-03 03:33:00,136] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-07-03 03:33:00,210] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:118: UserWarning: onnxruntime training package info: package_name: onnxruntime-training\n",
      "  warnings.warn(\"onnxruntime training package info: package_name: %s\" % package_name)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:119: UserWarning: onnxruntime training package info: __version__: 1.17.1\n",
      "  warnings.warn(\"onnxruntime training package info: __version__: %s\" % version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:120: UserWarning: onnxruntime training package info: cuda_version: 12.2\n",
      "  warnings.warn(\"onnxruntime training package info: cuda_version: %s\" % cuda_version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:121: UserWarning: onnxruntime build info: cudart_version: 12020\n",
      "  warnings.warn(\"onnxruntime build info: cudart_version: %s\" % cudart_version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:129: UserWarning: WARNING: failed to find cudart version that matches onnxruntime build info\n",
      "  warnings.warn(\"WARNING: failed to find cudart version that matches onnxruntime build info\")\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:130: UserWarning: WARNING: found cudart versions: [12010]\n",
      "  warnings.warn(\"WARNING: found cudart versions: %s\" % local_cudart_versions)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:118: UserWarning: onnxruntime training package info: package_name: onnxruntime-training\n",
      "  warnings.warn(\"onnxruntime training package info: package_name: %s\" % package_name)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:119: UserWarning: onnxruntime training package info: __version__: 1.17.1\n",
      "  warnings.warn(\"onnxruntime training package info: __version__: %s\" % version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:120: UserWarning: onnxruntime training package info: cuda_version: 12.2\n",
      "  warnings.warn(\"onnxruntime training package info: cuda_version: %s\" % cuda_version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:121: UserWarning: onnxruntime build info: cudart_version: 12020\n",
      "  warnings.warn(\"onnxruntime build info: cudart_version: %s\" % cudart_version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:129: UserWarning: WARNING: failed to find cudart version that matches onnxruntime build info\n",
      "  warnings.warn(\"WARNING: failed to find cudart version that matches onnxruntime build info\")\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:130: UserWarning: WARNING: found cudart versions: [12010]\n",
      "  warnings.warn(\"WARNING: found cudart versions: %s\" % local_cudart_versions)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:118: UserWarning: onnxruntime training package info: package_name: onnxruntime-training\n",
      "  warnings.warn(\"onnxruntime training package info: package_name: %s\" % package_name)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:119: UserWarning: onnxruntime training package info: __version__: 1.17.1\n",
      "  warnings.warn(\"onnxruntime training package info: __version__: %s\" % version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:120: UserWarning: onnxruntime training package info: cuda_version: 12.2\n",
      "  warnings.warn(\"onnxruntime training package info: cuda_version: %s\" % cuda_version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:121: UserWarning: onnxruntime build info: cudart_version: 12020\n",
      "  warnings.warn(\"onnxruntime build info: cudart_version: %s\" % cudart_version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:129: UserWarning: WARNING: failed to find cudart version that matches onnxruntime build info\n",
      "  warnings.warn(\"WARNING: failed to find cudart version that matches onnxruntime build info\")\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:130: UserWarning: WARNING: found cudart versions: [12010]\n",
      "  warnings.warn(\"WARNING: found cudart versions: %s\" % local_cudart_versions)\n",
      "World size: 3, rank: 0, local rank: 0\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "Namespace(load_from='/cosmos/local/IndexQuality/FinetuneLLM/Mixtral-8x7B-Instruct-v0.1/', model='mixtral', num_workers=24, training_data_from='/cosmos/local/IndexQuality/FinetuneLLM/TrainingData/O1_A3_crowd_llm_training_data_no_eval_dataset_overlap.tsv', eval_data_from='/cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/Mixtral_New_ym_bs_10_test/scrapekr1.2_spamllm2.4_sample_20.parquet', batch_size=1, gradient_accumulation_steps=2, num_epochs=2, save_checkpoint_steps=20, lr=1e-05, warmup=0.1, output_dir='/cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/Mixtral_New_ym_bs_10_test', experiment_name='MixtralTrain_ym', disable_tensorboard=True, log_dir='/cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/Mixtral_New_ym_bs_10_test/log')\n",
      "World size: 3, rank: 1, local rank: 1\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "World size: 3, rank: 2, local rank: 2\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "Loading checkpoint shards: 100%|████████████████| 19/19 [00:21<00:00,  1.15s/it]\n",
      "Model auto wrap policy:  functools.partial(<function transformer_auto_wrap_policy at 0x7f23b2d905e0>, transformer_layer_cls={<class 'transformers.models.mixtral.modeling_mixtral.MixtralDecoderLayer'>, <class 'torch.nn.modules.sparse.Embedding'>})\n",
      "Loading checkpoint shards: 100%|████████████████| 19/19 [00:28<00:00,  1.52s/it]\n",
      "model config: MixtralConfig {\n",
      "  \"_name_or_path\": \"/cosmos/local/IndexQuality/FinetuneLLM/Mixtral-8x7B-Instruct-v0.1/\",\n",
      "  \"architectures\": [\n",
      "    \"MixtralForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mixtral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_experts_per_tok\": 2,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"num_local_experts\": 8,\n",
      "  \"output_router_logits\": false,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"router_aux_loss_coef\": 0.02,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "MixtralForCausalLM(\n",
      "  (model): MixtralModel(\n",
      "    (embed_tokens): Embedding(32000, 4096)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MixtralDecoderLayer(\n",
      "        (self_attn): MixtralSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MixtralRotaryEmbedding()\n",
      "        )\n",
      "        (block_sparse_moe): MixtralSparseMoeBlock(\n",
      "          (gate): Linear(in_features=4096, out_features=8, bias=False)\n",
      "          (experts): ModuleList(\n",
      "            (0-7): 8 x MixtralBlockSparseTop2MLP(\n",
      "              (w1): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "              (w2): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "              (w3): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (input_layernorm): MixtralRMSNorm()\n",
      "        (post_attention_layernorm): MixtralRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): MixtralRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
      ")\n",
      "Model auto wrap policy:  functools.partial(<function transformer_auto_wrap_policy at 0x7fea2f3545e0>, transformer_layer_cls={<class 'transformers.models.mixtral.modeling_mixtral.MixtralDecoderLayer'>, <class 'torch.nn.modules.sparse.Embedding'>})\n",
      "Loading checkpoint shards: 100%|████████████████| 19/19 [00:29<00:00,  1.55s/it]\n",
      "Model auto wrap policy:  functools.partial(<function transformer_auto_wrap_policy at 0x7fb6158005e0>, transformer_layer_cls={<class 'torch.nn.modules.sparse.Embedding'>, <class 'transformers.models.mixtral.modeling_mixtral.MixtralDecoderLayer'>})\n",
      "init tensorboard, MixtralTrain_ym\n",
      "Epoch 0/2:   0%|                                      | 0/97933 [00:00<?, ?it/s]node-0:544038:544038 [0] NCCL INFO Bootstrap : Using eth0:10.6.35.182<0>\n",
      "node-0:544038:544038 [0] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.\n",
      "node-0:544038:544038 [0] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)\n",
      "node-0:544038:544038 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\n",
      "node-0:544038:544038 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.\n",
      "node-0:544038:544038 [0] NCCL INFO cudaDriverVersion 12020\n",
      "NCCL version 2.19.4+cuda12.1\n",
      "node-0:544039:544039 [1] NCCL INFO cudaDriverVersion 12020\n",
      "node-0:544039:544039 [1] NCCL INFO Bootstrap : Using eth0:10.6.35.182<0>\n",
      "node-0:544040:544040 [2] NCCL INFO cudaDriverVersion 12020\n",
      "node-0:544040:544040 [2] NCCL INFO Bootstrap : Using eth0:10.6.35.182<0>\n",
      "node-0:544039:544039 [1] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.\n",
      "node-0:544039:544039 [1] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)\n",
      "node-0:544039:544039 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\n",
      "node-0:544039:544039 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.\n",
      "node-0:544040:544040 [2] NCCL INFO NET/Plugin: Failed to find ncclNetPlugin_v7 symbol.\n",
      "node-0:544040:544040 [2] NCCL INFO NET/Plugin: Loaded net plugin NCCL RDMA Plugin v6 (v6)\n",
      "node-0:544040:544040 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.\n",
      "node-0:544040:544040 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (>= v5). ncclCollNetPlugin symbols v4 and lower are not supported.\n",
      "node-0:544038:556099 [0] NCCL INFO Plugin Path : /opt/nccl-rdma-sharp-plugins/lib/libnccl-net.so\n",
      "node-0:544038:556099 [0] NCCL INFO P2P plugin IBext\n",
      "\n",
      "node-0:544038:556099 [0] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:544038:556099 [0] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_0\n",
      "node-0:544038:556099 [0] NCCL INFO NET/IB : No device found.\n",
      "\n",
      "node-0:544038:556099 [0] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:544038:556099 [0] transport/net_ib.cc:193 NCCL WARN NET/IB : Unable to open device mlx5_0\n",
      "node-0:544038:556099 [0] NCCL INFO NET/IB : No device found.\n",
      "node-0:544038:556099 [0] NCCL INFO NET/Socket : Using [0]eth0:10.6.35.182<0>\n",
      "node-0:544038:556099 [0] NCCL INFO Using non-device net plugin version 0\n",
      "node-0:544038:556099 [0] NCCL INFO Using network Socket\n",
      "node-0:544039:556101 [1] NCCL INFO Plugin Path : /opt/nccl-rdma-sharp-plugins/lib/libnccl-net.so\n",
      "node-0:544039:556101 [1] NCCL INFO P2P plugin IBext\n",
      "\n",
      "node-0:544039:556101 [1] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:544039:556101 [1] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_0\n",
      "node-0:544039:556101 [1] NCCL INFO NET/IB : No device found.\n",
      "\n",
      "node-0:544039:556101 [1] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:544039:556101 [1] transport/net_ib.cc:193 NCCL WARN NET/IB : Unable to open device mlx5_0\n",
      "node-0:544039:556101 [1] NCCL INFO NET/IB : No device found.\n",
      "node-0:544039:556101 [1] NCCL INFO NET/Socket : Using [0]eth0:10.6.35.182<0>\n",
      "node-0:544039:556101 [1] NCCL INFO Using non-device net plugin version 0\n",
      "node-0:544039:556101 [1] NCCL INFO Using network Socket\n",
      "node-0:544040:556103 [2] NCCL INFO Plugin Path : /opt/nccl-rdma-sharp-plugins/lib/libnccl-net.so\n",
      "node-0:544040:556103 [2] NCCL INFO P2P plugin IBext\n",
      "\n",
      "node-0:544040:556103 [2] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:544040:556103 [2] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_0\n",
      "node-0:544040:556103 [2] NCCL INFO NET/IB : No device found.\n",
      "\n",
      "node-0:544040:556103 [2] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:544040:556103 [2] transport/net_ib.cc:193 NCCL WARN NET/IB : Unable to open device mlx5_0\n",
      "node-0:544040:556103 [2] NCCL INFO NET/IB : No device found.\n",
      "node-0:544040:556103 [2] NCCL INFO NET/Socket : Using [0]eth0:10.6.35.182<0>\n",
      "node-0:544040:556103 [2] NCCL INFO Using non-device net plugin version 0\n",
      "node-0:544040:556103 [2] NCCL INFO Using network Socket\n",
      "node-0:544040:556103 [2] NCCL INFO comm 0x89ec8800 rank 2 nranks 3 cudaDev 2 nvmlDev 2 busId 300000 commId 0x794ee80c65326b34 - Init START\n",
      "node-0:544039:556101 [1] NCCL INFO comm 0xd5fed360 rank 1 nranks 3 cudaDev 1 nvmlDev 1 busId 200000 commId 0x794ee80c65326b34 - Init START\n",
      "node-0:544038:556099 [0] NCCL INFO comm 0x88c3a830 rank 0 nranks 3 cudaDev 0 nvmlDev 0 busId 100000 commId 0x794ee80c65326b34 - Init START\n",
      "node-0:544040:556103 [2] NCCL INFO Topology detection : could not read /sys/devices/LNXSYSTM:00/LNXSYBUS:00/ACPI0004:00/VMBUS:00/00000041-0001-0000-3130-444532304235/pci0001:00/0001:00:00.0/../max_link_speed, ignoring\n",
      "node-0:544039:556101 [1] NCCL INFO Topology detection : could not read /sys/devices/LNXSYSTM:00/LNXSYBUS:00/ACPI0004:00/VMBUS:00/00000041-0001-0000-3130-444532304235/pci0001:00/0001:00:00.0/../max_link_speed, ignoring\n",
      "node-0:544038:556099 [0] NCCL INFO Topology detection : could not read /sys/devices/LNXSYSTM:00/LNXSYBUS:00/ACPI0004:00/VMBUS:00/00000041-0001-0000-3130-444532304235/pci0001:00/0001:00:00.0/../max_link_speed, ignoring\n",
      "node-0:544040:556103 [2] NCCL INFO Topology detection : could not read /sys/devices/LNXSYSTM:00/LNXSYBUS:00/ACPI0004:00/VMBUS:00/00000041-0001-0000-3130-444532304235/pci0001:00/0001:00:00.0/../max_link_width, ignoring\n",
      "node-0:544039:556101 [1] NCCL INFO Topology detection : could not read /sys/devices/LNXSYSTM:00/LNXSYBUS:00/ACPI0004:00/VMBUS:00/00000041-0001-0000-3130-444532304235/pci0001:00/0001:00:00.0/../max_link_width, ignoring\n",
      "node-0:544038:556099 [0] NCCL INFO Topology detection : could not read /sys/devices/LNXSYSTM:00/LNXSYBUS:00/ACPI0004:00/VMBUS:00/00000041-0001-0000-3130-444532304235/pci0001:00/0001:00:00.0/../max_link_width, ignoring\n",
      "node-0:544040:556103 [2] NCCL INFO Topology detection : could not read /sys/devices/LNXSYSTM:00/LNXSYBUS:00/ACPI0004:00/VMBUS:00/00000001-0002-0001-3130-444532304235/pci0002:00/0002:00:00.0/../max_link_speed, ignoring\n",
      "node-0:544040:556103 [2] NCCL INFO Topology detection : could not read /sys/devices/LNXSYSTM:00/LNXSYBUS:00/ACPI0004:00/VMBUS:00/00000001-0002-0001-3130-444532304235/pci0002:00/0002:00:00.0/../max_link_width, ignoring\n",
      "node-0:544039:556101 [1] NCCL INFO Topology detection : could not read /sys/devices/LNXSYSTM:00/LNXSYBUS:00/ACPI0004:00/VMBUS:00/00000001-0002-0001-3130-444532304235/pci0002:00/0002:00:00.0/../max_link_speed, ignoring\n",
      "node-0:544039:556101 [1] NCCL INFO Topology detection : could not read /sys/devices/LNXSYSTM:00/LNXSYBUS:00/ACPI0004:00/VMBUS:00/00000001-0002-0001-3130-444532304235/pci0002:00/0002:00:00.0/../max_link_width, ignoring\n",
      "node-0:544038:556099 [0] NCCL INFO Topology detection : could not read /sys/devices/LNXSYSTM:00/LNXSYBUS:00/ACPI0004:00/VMBUS:00/00000001-0002-0001-3130-444532304235/pci0002:00/0002:00:00.0/../max_link_speed, ignoring\n",
      "node-0:544038:556099 [0] NCCL INFO Topology detection : could not read /sys/devices/LNXSYSTM:00/LNXSYBUS:00/ACPI0004:00/VMBUS:00/00000001-0002-0001-3130-444532304235/pci0002:00/0002:00:00.0/../max_link_width, ignoring\n",
      "node-0:544040:556103 [2] NCCL INFO Topology detection : could not read /sys/devices/LNXSYSTM:00/LNXSYBUS:00/ACPI0004:00/VMBUS:00/000000c1-0003-0002-3130-444532304235/pci0003:00/0003:00:00.0/../max_link_speed, ignoring\n",
      "node-0:544040:556103 [2] NCCL INFO Topology detection : could not read /sys/devices/LNXSYSTM:00/LNXSYBUS:00/ACPI0004:00/VMBUS:00/000000c1-0003-0002-3130-444532304235/pci0003:00/0003:00:00.0/../max_link_width, ignoring\n",
      "node-0:544039:556101 [1] NCCL INFO Topology detection : could not read /sys/devices/LNXSYSTM:00/LNXSYBUS:00/ACPI0004:00/VMBUS:00/000000c1-0003-0002-3130-444532304235/pci0003:00/0003:00:00.0/../max_link_speed, ignoring\n",
      "node-0:544039:556101 [1] NCCL INFO Topology detection : could not read /sys/devices/LNXSYSTM:00/LNXSYBUS:00/ACPI0004:00/VMBUS:00/000000c1-0003-0002-3130-444532304235/pci0003:00/0003:00:00.0/../max_link_width, ignoring\n",
      "node-0:544038:556099 [0] NCCL INFO Topology detection : could not read /sys/devices/LNXSYSTM:00/LNXSYBUS:00/ACPI0004:00/VMBUS:00/000000c1-0003-0002-3130-444532304235/pci0003:00/0003:00:00.0/../max_link_speed, ignoring\n",
      "node-0:544038:556099 [0] NCCL INFO Topology detection : could not read /sys/devices/LNXSYSTM:00/LNXSYBUS:00/ACPI0004:00/VMBUS:00/000000c1-0003-0002-3130-444532304235/pci0003:00/0003:00:00.0/../max_link_width, ignoring\n",
      "node-0:544040:556103 [2] NCCL INFO === System : maxBw 240.0 totalBw 240.0 ===\n",
      "node-0:544040:556103 [2] NCCL INFO CPU/0 (1/2/-1)\n",
      "node-0:544040:556103 [2] NCCL INFO + PCI[5000.0] - NIC/0\n",
      "node-0:544040:556103 [2] NCCL INFO + PCI[24.0] - GPU/100000 (0)\n",
      "node-0:544040:556103 [2] NCCL INFO               + NVL[240.0] - GPU/200000\n",
      "node-0:544040:556103 [2] NCCL INFO + SYS[16.0] - CPU/1\n",
      "node-0:544040:556103 [2] NCCL INFO + SYS[16.0] - CPU/2\n",
      "node-0:544040:556103 [2] NCCL INFO CPU/1 (1/2/-1)\n",
      "node-0:544040:556103 [2] NCCL INFO + PCI[24.0] - GPU/200000 (1)\n",
      "node-0:544040:556103 [2] NCCL INFO               + NVL[240.0] - GPU/100000\n",
      "node-0:544040:556103 [2] NCCL INFO + SYS[16.0] - CPU/0\n",
      "node-0:544040:556103 [2] NCCL INFO + SYS[16.0] - CPU/2\n",
      "node-0:544040:556103 [2] NCCL INFO CPU/2 (1/2/-1)\n",
      "node-0:544040:556103 [2] NCCL INFO + PCI[24.0] - GPU/300000 (2)\n",
      "node-0:544040:556103 [2] NCCL INFO + SYS[16.0] - CPU/0\n",
      "node-0:544040:556103 [2] NCCL INFO + SYS[16.0] - CPU/1\n",
      "node-0:544040:556103 [2] NCCL INFO ==========================================\n",
      "node-0:544040:556103 [2] NCCL INFO GPU/100000 :GPU/100000 (0/5000.000000/LOC) GPU/200000 (1/240.000000/NVL) GPU/300000 (3/16.000000/SYS) CPU/0 (1/24.000000/PHB) CPU/1 (2/24.000000/PHB) CPU/2 (2/16.000000/SYS) \n",
      "node-0:544040:556103 [2] NCCL INFO GPU/200000 :GPU/100000 (1/240.000000/NVL) GPU/200000 (0/5000.000000/LOC) GPU/300000 (3/16.000000/SYS) CPU/0 (2/24.000000/PHB) CPU/1 (1/24.000000/PHB) CPU/2 (2/16.000000/SYS) \n",
      "node-0:544040:556103 [2] NCCL INFO GPU/300000 :GPU/100000 (3/16.000000/SYS) GPU/200000 (3/16.000000/SYS) GPU/300000 (0/5000.000000/LOC) CPU/0 (2/16.000000/SYS) CPU/1 (2/16.000000/SYS) CPU/2 (1/24.000000/PHB) \n",
      "node-0:544040:556103 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00000000\n",
      "node-0:544040:556103 [2] NCCL INFO NVLS multicast support is not available on dev 2\n",
      "node-0:544040:556103 [2] NCCL INFO Pattern 4, crossNic 0, nChannels 1, bw 15.000000/15.000000, type SYS/PIX, sameChannels 1\n",
      "node-0:544040:556103 [2] NCCL INFO  0 : GPU/0 GPU/1 GPU/2\n",
      "node-0:544040:556103 [2] NCCL INFO Pattern 1, crossNic 0, nChannels 1, bw 15.000000/15.000000, type SYS/PIX, sameChannels 1\n",
      "node-0:544040:556103 [2] NCCL INFO  0 : GPU/0 GPU/1 GPU/2\n",
      "node-0:544039:556101 [1] NCCL INFO === System : maxBw 240.0 totalBw 240.0 ===\n",
      "node-0:544039:556101 [1] NCCL INFO CPU/0 (1/2/-1)\n",
      "node-0:544039:556101 [1] NCCL INFO + PCI[5000.0] - NIC/0\n",
      "node-0:544039:556101 [1] NCCL INFO + PCI[24.0] - GPU/100000 (0)\n",
      "node-0:544039:556101 [1] NCCL INFO               + NVL[240.0] - GPU/200000\n",
      "node-0:544039:556101 [1] NCCL INFO + SYS[16.0] - CPU/1\n",
      "node-0:544039:556101 [1] NCCL INFO + SYS[16.0] - CPU/2\n",
      "node-0:544039:556101 [1] NCCL INFO CPU/1 (1/2/-1)\n",
      "node-0:544039:556101 [1] NCCL INFO + PCI[24.0] - GPU/200000 (1)\n",
      "node-0:544039:556101 [1] NCCL INFO               + NVL[240.0] - GPU/100000\n",
      "node-0:544039:556101 [1] NCCL INFO + SYS[16.0] - CPU/0\n",
      "node-0:544039:556101 [1] NCCL INFO + SYS[16.0] - CPU/2\n",
      "node-0:544039:556101 [1] NCCL INFO CPU/2 (1/2/-1)\n",
      "node-0:544039:556101 [1] NCCL INFO + PCI[24.0] - GPU/300000 (2)\n",
      "node-0:544039:556101 [1] NCCL INFO + SYS[16.0] - CPU/0\n",
      "node-0:544039:556101 [1] NCCL INFO + SYS[16.0] - CPU/1\n",
      "node-0:544039:556101 [1] NCCL INFO ==========================================\n",
      "node-0:544039:556101 [1] NCCL INFO GPU/100000 :GPU/100000 (0/5000.000000/LOC) GPU/200000 (1/240.000000/NVL) GPU/300000 (3/16.000000/SYS) CPU/0 (1/24.000000/PHB) CPU/1 (2/24.000000/PHB) CPU/2 (2/16.000000/SYS) \n",
      "node-0:544039:556101 [1] NCCL INFO GPU/200000 :GPU/100000 (1/240.000000/NVL) GPU/200000 (0/5000.000000/LOC) GPU/300000 (3/16.000000/SYS) CPU/0 (2/24.000000/PHB) CPU/1 (1/24.000000/PHB) CPU/2 (2/16.000000/SYS) \n",
      "node-0:544039:556101 [1] NCCL INFO GPU/300000 :GPU/100000 (3/16.000000/SYS) GPU/200000 (3/16.000000/SYS) GPU/300000 (0/5000.000000/LOC) CPU/0 (2/16.000000/SYS) CPU/1 (2/16.000000/SYS) CPU/2 (1/24.000000/PHB) \n",
      "node-0:544039:556101 [1] NCCL INFO Setting affinity for GPU 1 to ffff,ff000000\n",
      "node-0:544039:556101 [1] NCCL INFO NVLS multicast support is not available on dev 1\n",
      "node-0:544038:556099 [0] NCCL INFO === System : maxBw 240.0 totalBw 240.0 ===\n",
      "node-0:544038:556099 [0] NCCL INFO CPU/0 (1/2/-1)\n",
      "node-0:544038:556099 [0] NCCL INFO + PCI[5000.0] - NIC/0\n",
      "node-0:544038:556099 [0] NCCL INFO + PCI[24.0] - GPU/100000 (0)\n",
      "node-0:544038:556099 [0] NCCL INFO               + NVL[240.0] - GPU/200000\n",
      "node-0:544038:556099 [0] NCCL INFO + SYS[16.0] - CPU/1\n",
      "node-0:544038:556099 [0] NCCL INFO + SYS[16.0] - CPU/2\n",
      "node-0:544038:556099 [0] NCCL INFO CPU/1 (1/2/-1)\n",
      "node-0:544038:556099 [0] NCCL INFO + PCI[24.0] - GPU/200000 (1)\n",
      "node-0:544038:556099 [0] NCCL INFO               + NVL[240.0] - GPU/100000\n",
      "node-0:544038:556099 [0] NCCL INFO + SYS[16.0] - CPU/0\n",
      "node-0:544038:556099 [0] NCCL INFO + SYS[16.0] - CPU/2\n",
      "node-0:544038:556099 [0] NCCL INFO CPU/2 (1/2/-1)\n",
      "node-0:544038:556099 [0] NCCL INFO + PCI[24.0] - GPU/300000 (2)\n",
      "node-0:544038:556099 [0] NCCL INFO + SYS[16.0] - CPU/0\n",
      "node-0:544038:556099 [0] NCCL INFO + SYS[16.0] - CPU/1\n",
      "node-0:544038:556099 [0] NCCL INFO ==========================================\n",
      "node-0:544038:556099 [0] NCCL INFO GPU/100000 :GPU/100000 (0/5000.000000/LOC) GPU/200000 (1/240.000000/NVL) GPU/300000 (3/16.000000/SYS) CPU/0 (1/24.000000/PHB) CPU/1 (2/24.000000/PHB) CPU/2 (2/16.000000/SYS) \n",
      "node-0:544038:556099 [0] NCCL INFO GPU/200000 :GPU/100000 (1/240.000000/NVL) GPU/200000 (0/5000.000000/LOC) GPU/300000 (3/16.000000/SYS) CPU/0 (2/24.000000/PHB) CPU/1 (1/24.000000/PHB) CPU/2 (2/16.000000/SYS) \n",
      "node-0:544038:556099 [0] NCCL INFO GPU/300000 :GPU/100000 (3/16.000000/SYS) GPU/200000 (3/16.000000/SYS) GPU/300000 (0/5000.000000/LOC) CPU/0 (2/16.000000/SYS) CPU/1 (2/16.000000/SYS) CPU/2 (1/24.000000/PHB) \n",
      "node-0:544038:556099 [0] NCCL INFO Setting affinity for GPU 0 to ffffff\n",
      "node-0:544038:556099 [0] NCCL INFO NVLS multicast support is not available on dev 0\n",
      "node-0:544038:556099 [0] NCCL INFO Pattern 4, crossNic 0, nChannels 1, bw 15.000000/15.000000, type SYS/PIX, sameChannels 1\n",
      "node-0:544038:556099 [0] NCCL INFO  0 : GPU/0 GPU/1 GPU/2\n",
      "node-0:544038:556099 [0] NCCL INFO Pattern 1, crossNic 0, nChannels 1, bw 15.000000/15.000000, type SYS/PIX, sameChannels 1\n",
      "node-0:544038:556099 [0] NCCL INFO  0 : GPU/0 GPU/1 GPU/2\n",
      "node-0:544039:556101 [1] NCCL INFO Pattern 4, crossNic 0, nChannels 1, bw 15.000000/15.000000, type SYS/PIX, sameChannels 1\n",
      "node-0:544039:556101 [1] NCCL INFO  0 : GPU/0 GPU/1 GPU/2\n",
      "node-0:544039:556101 [1] NCCL INFO Pattern 1, crossNic 0, nChannels 1, bw 15.000000/15.000000, type SYS/PIX, sameChannels 1\n",
      "node-0:544039:556101 [1] NCCL INFO  0 : GPU/0 GPU/1 GPU/2\n",
      "node-0:544038:556099 [0] NCCL INFO Tree 0 : -1 -> 0 -> 1/-1/-1\n",
      "node-0:544038:556099 [0] NCCL INFO Tree 1 : -1 -> 0 -> 1/-1/-1\n",
      "node-0:544039:556101 [1] NCCL INFO Tree 0 : 0 -> 1 -> 2/-1/-1\n",
      "node-0:544039:556101 [1] NCCL INFO Tree 1 : 0 -> 1 -> 2/-1/-1\n",
      "node-0:544040:556103 [2] NCCL INFO Ring 00 : 1 -> 2 -> 0\n",
      "node-0:544040:556103 [2] NCCL INFO Ring 01 : 1 -> 2 -> 0\n",
      "node-0:544040:556103 [2] NCCL INFO Trees [0] -1/-1/-1->2->1 [1] -1/-1/-1->2->1\n",
      "node-0:544040:556103 [2] NCCL INFO P2P Chunksize set to 524288\n",
      "node-0:544038:556099 [0] NCCL INFO Channel 00/02 :    0   1   2\n",
      "node-0:544038:556099 [0] NCCL INFO Channel 01/02 :    0   1   2\n",
      "node-0:544038:556099 [0] NCCL INFO Ring 00 : 2 -> 0 -> 1\n",
      "node-0:544038:556099 [0] NCCL INFO Ring 01 : 2 -> 0 -> 1\n",
      "node-0:544038:556099 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1\n",
      "node-0:544038:556099 [0] NCCL INFO P2P Chunksize set to 524288\n",
      "node-0:544039:556101 [1] NCCL INFO Ring 00 : 0 -> 1 -> 2\n",
      "node-0:544039:556101 [1] NCCL INFO Ring 01 : 0 -> 1 -> 2\n",
      "node-0:544039:556101 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0\n",
      "node-0:544039:556101 [1] NCCL INFO P2P Chunksize set to 524288\n",
      "node-0:544040:556103 [2] NCCL INFO Rank 2 selecting transport for rank 1\n",
      "node-0:544040:556103 [2] NCCL INFO Transport 0 canConnect 0\n",
      "node-0:544040:556103 [2] NCCL INFO Transport 1 canConnect 1\n",
      "node-0:544039:556101 [1] NCCL INFO Rank 1 selecting transport for rank 0\n",
      "node-0:544039:556101 [1] NCCL INFO Transport 0 canConnect 1\n",
      "node-0:544038:556099 [0] NCCL INFO Rank 0 selecting transport for rank 2\n",
      "node-0:544038:556099 [0] NCCL INFO Transport 0 canConnect 0\n",
      "node-0:544038:556099 [0] NCCL INFO Transport 1 canConnect 1\n",
      "node-0:544039:556101 [1] NCCL INFO Rank 1 selecting transport for rank 0\n",
      "node-0:544039:556101 [1] NCCL INFO Transport 0 canConnect 1\n",
      "node-0:544039:556101 [1] NCCL INFO Rank 1 selecting transport for rank 2\n",
      "node-0:544039:556101 [1] NCCL INFO Transport 0 canConnect 0\n",
      "node-0:544039:556101 [1] NCCL INFO Transport 1 canConnect 1\n",
      "node-0:544039:556101 [1] NCCL INFO Channel 00 : 1[1] -> 2[2] via SHM/direct/direct\n",
      "node-0:544039:556101 [1] NCCL INFO Rank 1 selecting transport for rank 2\n",
      "node-0:544039:556101 [1] NCCL INFO Transport 0 canConnect 0\n",
      "node-0:544039:556101 [1] NCCL INFO Transport 1 canConnect 1\n",
      "node-0:544039:556101 [1] NCCL INFO Channel 01 : 1[1] -> 2[2] via SHM/direct/direct\n",
      "node-0:544040:556103 [2] NCCL INFO Rank 2 selecting transport for rank 1\n",
      "node-0:544040:556103 [2] NCCL INFO Transport 0 canConnect 0\n",
      "node-0:544040:556103 [2] NCCL INFO Transport 1 canConnect 1\n",
      "node-0:544038:556099 [0] NCCL INFO Rank 0 selecting transport for rank 2\n",
      "node-0:544038:556099 [0] NCCL INFO Transport 0 canConnect 0\n",
      "node-0:544038:556099 [0] NCCL INFO Transport 1 canConnect 1\n",
      "node-0:544040:556103 [2] NCCL INFO Rank 2 selecting transport for rank 0\n",
      "node-0:544040:556103 [2] NCCL INFO Transport 0 canConnect 0\n",
      "node-0:544040:556103 [2] NCCL INFO Transport 1 canConnect 1\n",
      "node-0:544040:556103 [2] NCCL INFO Channel 00 : 2[2] -> 0[0] via SHM/direct/direct\n",
      "node-0:544040:556103 [2] NCCL INFO Rank 2 selecting transport for rank 0\n",
      "node-0:544040:556103 [2] NCCL INFO Transport 0 canConnect 0\n",
      "node-0:544040:556103 [2] NCCL INFO Transport 1 canConnect 1\n",
      "node-0:544040:556103 [2] NCCL INFO Channel 01 : 2[2] -> 0[0] via SHM/direct/direct\n",
      "node-0:544038:556099 [0] NCCL INFO Rank 0 selecting transport for rank 1\n",
      "node-0:544038:556099 [0] NCCL INFO Transport 0 canConnect 1\n",
      "node-0:544038:556099 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "node-0:544038:556099 [0] NCCL INFO Rank 0 selecting transport for rank 1\n",
      "node-0:544038:556099 [0] NCCL INFO Transport 0 canConnect 1\n",
      "node-0:544038:556099 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM/read\n",
      "node-0:544040:556103 [2] NCCL INFO Connected all rings\n",
      "node-0:544040:556103 [2] NCCL INFO Rank 2 selecting transport for rank 1\n",
      "node-0:544040:556103 [2] NCCL INFO Transport 0 canConnect 0\n",
      "node-0:544040:556103 [2] NCCL INFO Transport 1 canConnect 1\n",
      "node-0:544038:556099 [0] NCCL INFO Connected all rings\n",
      "node-0:544038:556099 [0] NCCL INFO Rank 0 selecting transport for rank 1\n",
      "node-0:544038:556099 [0] NCCL INFO Transport 0 canConnect 1\n",
      "node-0:544039:556101 [1] NCCL INFO Connected all rings\n",
      "node-0:544039:556101 [1] NCCL INFO Rank 1 selecting transport for rank 2\n",
      "node-0:544039:556101 [1] NCCL INFO Transport 0 canConnect 0\n",
      "node-0:544039:556101 [1] NCCL INFO Transport 1 canConnect 1\n",
      "node-0:544040:556103 [2] NCCL INFO Channel 00 : 2[2] -> 1[1] via SHM/direct/direct\n",
      "node-0:544040:556103 [2] NCCL INFO Rank 2 selecting transport for rank 1\n",
      "node-0:544040:556103 [2] NCCL INFO Transport 0 canConnect 0\n",
      "node-0:544040:556103 [2] NCCL INFO Transport 1 canConnect 1\n",
      "node-0:544038:556099 [0] NCCL INFO Rank 0 selecting transport for rank 1\n",
      "node-0:544040:556103 [2] NCCL INFO Channel 01 : 2[2] -> 1[1] via SHM/direct/direct\n",
      "node-0:544038:556099 [0] NCCL INFO Transport 0 canConnect 1\n",
      "node-0:544039:556101 [1] NCCL INFO Rank 1 selecting transport for rank 2\n",
      "node-0:544039:556101 [1] NCCL INFO Transport 0 canConnect 0\n",
      "node-0:544039:556101 [1] NCCL INFO Transport 1 canConnect 1\n",
      "node-0:544039:556101 [1] NCCL INFO Rank 1 selecting transport for rank 0\n",
      "node-0:544039:556101 [1] NCCL INFO Transport 0 canConnect 1\n",
      "node-0:544039:556101 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "node-0:544039:556101 [1] NCCL INFO Rank 1 selecting transport for rank 0\n",
      "node-0:544039:556101 [1] NCCL INFO Transport 0 canConnect 1\n",
      "node-0:544039:556101 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/CUMEM/read\n",
      "node-0:544038:556099 [0] NCCL INFO Connected all trees\n",
      "node-0:544038:556099 [0] NCCL INFO threadThresholds 8/8/64 | 24/8/64 | 512 | 512\n",
      "node-0:544038:556099 [0] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\n",
      "node-0:544040:556103 [2] NCCL INFO Connected all trees\n",
      "node-0:544040:556103 [2] NCCL INFO threadThresholds 8/8/64 | 24/8/64 | 512 | 512\n",
      "node-0:544040:556103 [2] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\n",
      "node-0:544039:556101 [1] NCCL INFO Connected all trees\n",
      "node-0:544039:556101 [1] NCCL INFO threadThresholds 8/8/64 | 24/8/64 | 512 | 512\n",
      "node-0:544039:556101 [1] NCCL INFO 2 coll channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer\n",
      "node-0:544040:556103 [2] NCCL INFO MSCCL: No external scheduler found, using internal implementation\n",
      "node-0:544040:556103 [2] NCCL INFO MSCCL: Internal Scheduler will use /usr/lib/x86_64-linux-gnu/msccl-algorithms as algorithm directory and /usr/lib/x86_64-linux-gnu/../share/nccl/msccl-algorithms as share algorithm directory and /usr/share/nccl/msccl-algorithms as package installed share algorithm directory \n",
      "node-0:544040:556103 [2] NCCL INFO Using MSCCL Algo files from /usr/share/nccl/msccl-algorithms\n",
      "node-0:544039:556101 [1] NCCL INFO MSCCL: No external scheduler found, using internal implementation\n",
      "node-0:544038:556099 [0] NCCL INFO MSCCL: No external scheduler found, using internal implementation\n",
      "node-0:544039:556101 [1] NCCL INFO MSCCL: Internal Scheduler will use /usr/lib/x86_64-linux-gnu/msccl-algorithms as algorithm directory and /usr/lib/x86_64-linux-gnu/../share/nccl/msccl-algorithms as share algorithm directory and /usr/share/nccl/msccl-algorithms as package installed share algorithm directory \n",
      "node-0:544038:556099 [0] NCCL INFO MSCCL: Internal Scheduler will use /usr/lib/x86_64-linux-gnu/msccl-algorithms as algorithm directory and /usr/lib/x86_64-linux-gnu/../share/nccl/msccl-algorithms as share algorithm directory and /usr/share/nccl/msccl-algorithms as package installed share algorithm directory \n",
      "node-0:544039:556101 [1] NCCL INFO Using MSCCL Algo files from /usr/share/nccl/msccl-algorithms\n",
      "node-0:544038:556099 [0] NCCL INFO Using MSCCL Algo files from /usr/share/nccl/msccl-algorithms\n",
      "node-0:544040:556103 [2] NCCL INFO MSCCL: Initialization finished\n",
      "node-0:544038:556099 [0] NCCL INFO MSCCL: Initialization finished\n",
      "node-0:544039:556101 [1] NCCL INFO MSCCL: Initialization finished\n",
      "node-0:544040:556103 [2] NCCL INFO comm 0x89ec8800 rank 2 nranks 3 cudaDev 2 nvmlDev 2 busId 300000 commId 0x794ee80c65326b34 - Init COMPLETE\n",
      "node-0:544039:556101 [1] NCCL INFO comm 0xd5fed360 rank 1 nranks 3 cudaDev 1 nvmlDev 1 busId 200000 commId 0x794ee80c65326b34 - Init COMPLETE\n",
      "node-0:544038:556099 [0] NCCL INFO comm 0x88c3a830 rank 0 nranks 3 cudaDev 0 nvmlDev 0 busId 100000 commId 0x794ee80c65326b34 - Init COMPLETE\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "--->>> step: 0\n",
      "--->>> step: 0\n",
      "--->>> step: 0\n",
      "Epoch 0/2:   0%|                                      | 0/97933 [00:34<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/cosmos/local/IndexQuality/ContentModels/DataAugmentation/data/CBSpam_v3/Code/FinetuneLLM-US/new/train_script_optimized_test.py\", line 387, in <module>\n",
      "    train(args, *setup(args))\n",
      "  File \"/cosmos/local/IndexQuality/ContentModels/DataAugmentation/data/CBSpam_v3/Code/FinetuneLLM-US/new/train_script_optimized_test.py\", line 312, in train\n",
      "    optimizer.step()\n",
      "  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/optim/lr_scheduler.py\", line 75, in wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 385, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 76, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/optim/adamw.py\", line 176, in step\n",
      "    has_complex = self._init_group(\n",
      "  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/optim/adamw.py\", line 127, in _init_group\n",
      "    state[\"exp_avg_sq\"] = torch.zeros_like(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 924.00 MiB. GPU 2 has a total capacity of 79.15 GiB of which 405.31 MiB is free. Process 3817960 has 78.74 GiB memory in use. Of the allocated memory 74.15 GiB is allocated by PyTorch, and 3.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Traceback (most recent call last):\n",
      "  File \"/cosmos/local/IndexQuality/ContentModels/DataAugmentation/data/CBSpam_v3/Code/FinetuneLLM-US/new/train_script_optimized_test.py\", line 387, in <module>\n",
      "    train(args, *setup(args))\n",
      "  File \"/cosmos/local/IndexQuality/ContentModels/DataAugmentation/data/CBSpam_v3/Code/FinetuneLLM-US/new/train_script_optimized_test.py\", line 312, in train\n",
      "    optimizer.step()\n",
      "  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/optim/lr_scheduler.py\", line 75, in wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 385, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 76, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/optim/adamw.py\", line 176, in step\n",
      "    has_complex = self._init_group(\n",
      "  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/optim/adamw.py\", line 127, in _init_group\n",
      "    state[\"exp_avg_sq\"] = torch.zeros_like(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 924.00 MiB. GPU 0 has a total capacity of 79.15 GiB of which 355.31 MiB is free. Process 3817958 has 78.79 GiB memory in use. Of the allocated memory 74.15 GiB is allocated by PyTorch, and 3.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "Traceback (most recent call last):\n",
      "  File \"/cosmos/local/IndexQuality/ContentModels/DataAugmentation/data/CBSpam_v3/Code/FinetuneLLM-US/new/train_script_optimized_test.py\", line 387, in <module>\n",
      "    train(args, *setup(args))\n",
      "  File \"/cosmos/local/IndexQuality/ContentModels/DataAugmentation/data/CBSpam_v3/Code/FinetuneLLM-US/new/train_script_optimized_test.py\", line 312, in train\n",
      "    optimizer.step()\n",
      "  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/optim/lr_scheduler.py\", line 75, in wrapper\n",
      "    return wrapped(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 385, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 76, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/optim/adamw.py\", line 176, in step\n",
      "    has_complex = self._init_group(\n",
      "  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/optim/adamw.py\", line 127, in _init_group\n",
      "    state[\"exp_avg_sq\"] = torch.zeros_like(\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 924.00 MiB. GPU 1 has a total capacity of 79.15 GiB of which 355.31 MiB is free. Process 3817959 has 78.79 GiB memory in use. Of the allocated memory 74.15 GiB is allocated by PyTorch, and 3.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "[2024-07-03 03:34:56,986] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 544038) of binary: /opt/conda/envs/ptca/bin/python\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/ptca/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 347, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/distributed/run.py\", line 812, in main\n",
      "    run(args)\n",
      "  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/distributed/run.py\", line 803, in run\n",
      "    elastic_launch(\n",
      "  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 135, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 268, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "train_script_optimized_test.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "[1]:\n",
      "  time      : 2024-07-03_03:34:56\n",
      "  host      : node-0\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : 1 (pid: 544039)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "[2]:\n",
      "  time      : 2024-07-03_03:34:56\n",
      "  host      : node-0\n",
      "  rank      : 2 (local_rank: 2)\n",
      "  exitcode  : 1 (pid: 544040)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2024-07-03_03:34:56\n",
      "  host      : node-0\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 544038)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=\"0,1,2,3,4,5,6,7,8,9,10,11,12,13\"\n",
    "!torchrun --nnodes 1 --nproc_per_node 13 train_script_optimized_test3.py \\\n",
    "    --load_from /cosmos/local/IndexQuality/FinetuneLLM/Mixtral-8x7B-Instruct-v0.1/ \\\n",
    "    --model mixtral \\\n",
    "    --num_workers 4 \\\n",
    "    --training_data_from /cosmos/local/IndexQuality/FinetuneLLM/TrainingData/O1_A3_crowd_llm_training_data_no_eval_dataset_overlap.tsv \\\n",
    "    --batch_size 10 \\\n",
    "    --gradient_accumulation_steps 2\\\n",
    "    --num_epochs 2 \\\n",
    "    --save_checkpoint_steps 300 \\\n",
    "    --lr 1e-5 \\\n",
    "    --output_dir /cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/Mixtral_New_ym_bs_10_test \\\n",
    "    --experiment_name MixtralTrain_ym \\\n",
    "    --disable_tensorboard False \\\n",
    "    --log_dir /cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/Mixtral_New_ym_bs_10_test/log\n",
    "    \n",
    "# --eval_data_from /cosmos/local/IndexQuality/FinetuneLLM/EvaluationSets/scrapekr1.2_spamllm2.4.parquet \\\n",
    "# pip install transformers[torch]==4.38.1 datasets scikit-learn dataclasses lightgbm matplotlib mlflow tensorboard && cd /cosmos/local/IndexQuality/ContentModels/DataAugmentation/data/CBSpam_v3/Code/FinetuneLLM-US/new/ && CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 torchrun --nnodes 1 --nproc_per_node 16 train_script_again.py --output_dir /cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/Mixtral_New_ym_bs_10_again --experiment_name MixtralTrain_ym --load_from /cosmos/local/IndexQuality/FinetuneLLM/Mixtral-8x7B-Instruct-v0.1/ --disable_tensorflow False --log_dir /cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/Mixtral_New_ym_bs_10_again/log\n",
    "CUDA_VISIBLE_DEVICES=6,7 python -m torch.distributed.run --master_port 29501 --nnodes 1 --nproc_per_node 2 inf_test.py --batch_size {args.batch_size} --models_dir {checkpoint_dir} --eval_data_from {args.eval_data_from}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
