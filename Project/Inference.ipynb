{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference AutoModelForClassification & PromptClassification\n",
    "### AutoModelForClassification includes: E5, Qwen \n",
    "### PromptClassification: Mixtral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d232861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inf_test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inf_test.py\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import pickle\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig\n",
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "from peft import PeftModel\n",
    "\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "prompt_part1 = \\\n",
    "f'''You are a website spam expert. You are given information about a webpage to judge whether or not it is spam. 0 means nonspam and 1 means spam. Give your prediction after the <ANS>: tag.\n",
    "    Url: {{Url}}\n",
    "    UrlTitle: {{UrlTitle}}\n",
    "    UrlSnippet: {{UrlSnippet}} \n",
    "    Site Content: {{FullBody}}\n",
    "'''\n",
    "\n",
    "prompt_part2_inference = \\\n",
    "'''\n",
    "What is your prediction <ANS>: '''\n",
    "\n",
    "IGNORE_INDEX = -100  # The default setting in CrossEntropyLoss\n",
    "MAX_LENGTH_EVAL = 1024\n",
    "\n",
    "class EvalDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        def apply_prompt_template(sample):\n",
    "            return prompt_part1.format(Url=sample['Url'],\n",
    "                                        UrlTitle=sample['UrlTitle'],\n",
    "                                        UrlSnippet=sample['UrlSnippet'],\n",
    "                                        FullBody=sample['FullBody'])\n",
    "        row = self.df.iloc[idx]\n",
    "        text = apply_prompt_template(row)\n",
    "        \n",
    "        res = self.tokenizer(text, prompt_part2_inference, add_special_tokens=False, max_length=MAX_LENGTH_EVAL, padding='max_length', truncation='only_first')\n",
    "        return {\n",
    "            'input_ids': torch.tensor(res['input_ids']),\n",
    "            'attention_mask': torch.tensor(res['attention_mask'])\n",
    "        }\n",
    "\n",
    "def create_eval_dataloader(df, tokenizer, batch_size=16):\n",
    "    ds = EvalDataset(df, tokenizer)\n",
    "\n",
    "    dataloader = DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
    "    return dataloader\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--load_from', type=str, required=True)\n",
    "    # parser.add_argument('--tokenizer_dir', type=str, default='/data/local/IndexQuality/FinetuneLLM/Phi-3-medium')\n",
    "    parser.add_argument('--tokenizer_dir', type=str, default='/cosmos/local/IndexQuality/FinetuneLLM/Mixtral-8x7B-Instruct-v0.1/')\n",
    "    parser.add_argument('--input_file', type=str, required=True) \n",
    "    parser.add_argument('--output_file', type=str, required=True)\n",
    "    parser.add_argument('--batch_size', type=int, default=4)\n",
    "    parser.add_argument('--max_new_tokens', type=int, default=1)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    dist.init_process_group(\"nccl\")\n",
    "    world_size = dist.get_world_size()\n",
    "    local_rank = dist.get_rank()\n",
    "    print('local rank:', local_rank, torch.distributed.is_initialized(), world_size)\n",
    "    if local_rank == 0:\n",
    "        print(args)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_dir,add_bos_token=True,trust_remote_code=True)\n",
    "    # left padding for batch inference\n",
    "    tokenizer.padding_side = 'left'\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # df = pd.read_parquet(\"/data/local/IndexQuality/FinetuneLLM/EvaluationSets/scrapekr1.2_spamllm2.4.parquet\")[[\"Url\",\"UrlTitle\",\"UrlSnippet\",\"FullBody\"]]\n",
    "    # df = pd.read_csv(args.input_file,sep=\"\\t\")[[\"Url\",\"UrlTitle\",\"UrlSnippet\",\"FullBody\",\"UrlID\"]]\n",
    "    # df = pd.read_csv(args.input_file,sep=\"\\t\")[[\"Url\",\"UrlTitle\",\"UrlSnippet\",\"FullBody\"]]\n",
    "    # df = pd.read_parquet(args.input_file)[[\"Url\",\"UrlTitle\",\"UrlSnippet\",\"FullBody\"]]\n",
    "    # df = pd.read_parquet(args.input_file)\n",
    "    #df = pd.read_csv(args.input_file,sep=\"\\t\",lineterminator='\\n')[[\"Url\",\"UrlTitle\",\"UrlSnippet\",\"FullBody\"]]\n",
    "    #df = pd.read_parquet(\"/data/local/IndexQuality/FinetuneLLM/EvaluationSets/spamgtx5.0_UHRSoutput 1.parquet\")[[\"Url\",\"UrlTitle\",\"UrlSnippet\",\"FullBody\"]]\n",
    "\n",
    "    df = pd.read_parquet(\"/cosmos/local/IndexQuality/FinetuneLLM/EvaluationSets/scrapekr1.2_spamllm2.4.parquet\")[[\"Url\",\"UrlTitle\",\"UrlSnippet\",\"FullBody\"]]\n",
    "    # df = pd.read_csv(\"/cosmos/local/IndexQuality/FinetuneLLM/EvaluationSets/test_dataset_2024_03_05.tsv\",sep=\"\\t\")[[\"Url\",\"UrlTitle\",\"UrlSnippet\",\"FullBody\"]]\n",
    "    # df = pd.read_parquet(\"/cosmos/local/IndexQuality/FinetuneLLM/EvaluationSets/spamgtx5.0_UHRSoutput_201.parquet\")[[\"Url\",\"UrlTitle\",\"UrlSnippet\",\"FullBody\"]]\n",
    "    # df = pd.read_csv(\"/cosmos/local/IndexQuality/FinetuneLLM/EvaluationSets/Clean_60k.tsv\",sep=\"\\t\",lineterminator=\"\\n\").drop_duplicates()\n",
    "    # df = pd.read_csv(\"/cosmos/local/IndexQuality/FinetuneLLM/EvaluationSets/8k_with_flipped_labels.tsv\",sep=\"\\t\")\n",
    "    \n",
    "    if world_size > 1:\n",
    "        df_rank = np.array_split(df, world_size)[local_rank]\n",
    "    else:\n",
    "        df_rank = df\n",
    "    \n",
    "    print(df.shape, df_rank.shape)\n",
    "    \n",
    "    dataloader = create_eval_dataloader(df_rank, tokenizer, batch_size=args.batch_size)\n",
    "    \n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    \n",
    "    # config = AutoConfig.from_pretrained(args.tokenizer_dir)\n",
    "    # config.gradient_checkpointing = True\n",
    "    \n",
    "    # model = AutoModelForCausalLM.from_pretrained(args.load_from, device_map=f'cuda:{local_rank}',trust_remote_code=True, \n",
    "    # model = AutoModelForCausalLM.from_pretrained(\"/cosmos/local/IndexQuality/FinetuneLLM/FullTrain/Mixtral_2_6_host_site/\", \n",
    "    model = AutoModelForCausalLM.from_pretrained(args.load_from, \n",
    "                                                device_map=f'cuda:{local_rank}',\n",
    "                                                trust_remote_code=True, \n",
    "                                                quantization_config=quantization_config,\n",
    "                                                # config=config,\n",
    "                                                torch_dtype=torch.bfloat16,)\n",
    "    #load trained model\n",
    "    # model.load_state_dict(torch.load(\"/data/local/IndexQuality/FinetuneLLM/FullTrain/Phi3_Medium_O1_A3_data_low_lr_quantized/pytorch_model_2000.bin\"))\n",
    "    # model = model.to(torch.bfloat16)\n",
    "    if world_size > 1:\n",
    "        ddp_model = DDP(model, device_ids=[local_rank])\n",
    "    model.eval()\n",
    "\n",
    "    result_rank = []\n",
    "    prob_rank = []\n",
    "    \n",
    "    start = time.time()\n",
    "    for i, x in tqdm(enumerate(dataloader), total=len(dataloader), disable=(local_rank!=0)):\n",
    "        input_data = {key: value for key, value in x.items() if key in ['input_ids', 'attention_mask']}\n",
    "        model_inputs = BatchEncoding(input_data).to(f'cuda:{local_rank}')\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(**model_inputs, max_new_tokens=1,temperature=0.0,return_dict_in_generate=True, output_scores=True)\n",
    "            transition_scores = model.compute_transition_scores(output.sequences, output.scores, normalize_logits=True).to(\"cpu\")\n",
    "            generated_tokens = output.sequences.detach()[:, MAX_LENGTH_EVAL:].cpu()\n",
    "            score_length = transition_scores.shape[0]\n",
    "            result_rank.extend(tokenizer.batch_decode(generated_tokens, skip_special_tokens=True))\n",
    "            prob_rank.extend(np.exp(transition_scores.reshape(score_length).numpy()))\n",
    "            \n",
    "        if i % 100 == 0:\n",
    "            t = time.time() - start\n",
    "            n_samples = (i+1)*dataloader.batch_size\n",
    "            throughput = n_samples/t if t > 0 else 0\n",
    "            print(f'rank {local_rank}, total samples：{n_samples} throughput： {throughput:.2f} /s')\n",
    "        \n",
    "        if i == 0 and local_rank == 0:\n",
    "            p = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "            print(p)\n",
    "    \n",
    "    print('rank', local_rank, 'inference done', len(result_rank))\n",
    "    print(result_rank)\n",
    "    \n",
    "    print('rank', local_rank, 'inference done', len(prob_rank))\n",
    "    print(prob_rank)\n",
    "\n",
    "    df_rank['Prediction'] = result_rank\n",
    "    df_rank[\"Probability\"] = prob_rank\n",
    "    \n",
    "    if local_rank <= 0:\n",
    "        parent_dir = os.path.dirname(args.output_file)\n",
    "        if parent_dir and not os.path.exists(parent_dir):\n",
    "            os.makedirs(parent_dir, exist_ok=True)\n",
    "    \n",
    "    with open(args.output_file + f'_rank_{local_rank}.pkl', 'wb') as f:\n",
    "        pickle.dump(df_rank, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    if world_size > 1:\n",
    "        dist.barrier()\n",
    "        dfs = []\n",
    "        if local_rank <= 0:\n",
    "            print('Run time ',time.time() - start)\n",
    "            print('Merging results ...')\n",
    "            for i in range(world_size):\n",
    "                file_path = args.output_file + f'_rank_{i}.pkl'\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    dfs.append(pickle.load(f))\n",
    "                try:\n",
    "                    os.remove(file_path)\n",
    "                except OSError:\n",
    "                    pass\n",
    "            df_res = pd.concat(dfs, axis=0)\n",
    "    else:\n",
    "        df_res = df_rank\n",
    "    \n",
    "    if local_rank <= 0:\n",
    "        print('total result:', df_res.shape)\n",
    "        df_res.to_csv(args.output_file, index=False, sep='\\t')\n",
    "        print('saved to:', args.output_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Mixtral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdbfcd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inf_multi_mixtral_model_test_5.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inf_multi_mixtral_model_test_5.py\n",
    "# model_path\n",
    "# eval_set_path\n",
    "# prediction_result_path\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import pickle\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig\n",
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "from peft import PeftModel\n",
    "\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "import copy\n",
    "import pandas as pd\n",
    "import gc, ctypes, torch\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "prompt_part1 = \\\n",
    "f'''You are a website spam expert. You are given information about a webpage to judge whether or not it is spam. 0 means nonspam and 1 means spam. Give your prediction after the <ANS>: tag.\n",
    "    Url: {{Url}}\n",
    "    UrlTitle: {{UrlTitle}}\n",
    "    UrlSnippet: {{UrlSnippet}} \n",
    "    Site Content: {{FullBody}}\n",
    "'''\n",
    "\n",
    "prompt_part2_inference = \\\n",
    "'''\n",
    "What is your prediction <ANS>: '''\n",
    "\n",
    "IGNORE_INDEX = -100  # The default setting in CrossEntropyLoss\n",
    "# MAX_LENGTH_EVAL = 1024\n",
    "\n",
    "class EvalDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_seq_length):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        def apply_prompt_template(sample):\n",
    "            return prompt_part1.format(Url=sample['Url'],\n",
    "                                        UrlTitle=sample['UrlTitle'],\n",
    "                                        UrlSnippet=sample['UrlSnippet'],\n",
    "                                        FullBody=sample['FullBody'])\n",
    "        \n",
    "        row = self.df.iloc[idx]\n",
    "        text = apply_prompt_template(row)\n",
    "        \n",
    "        # res = self.tokenizer(text, prompt_part2_inference, add_special_tokens=False, max_length=MAX_LENGTH_EVAL, padding='max_length', truncation='only_first')\n",
    "        res = self.tokenizer(f\"{self.tokenizer.bos_token} {text}\", prompt_part2_inference, add_special_tokens=False, max_length=self.max_seq_length, padding='max_length', truncation='only_first')\n",
    "        return {\n",
    "            'input_ids': torch.tensor(res['input_ids']),\n",
    "            'attention_mask': torch.tensor(res['attention_mask']),\n",
    "        }\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--load_from', type=str, required=True)\n",
    "    parser.add_argument('--tokenizer_dir', type=str, default='/cosmos/local/IndexQuality/FinetuneLLM/Mixtral-8x7B-Instruct-v0.1/')\n",
    "    parser.add_argument('--input_file', type=str, required=True) \n",
    "    parser.add_argument('--output_file', type=str, required=True)\n",
    "    parser.add_argument('--batch_size', type=int, default=4)\n",
    "    parser.add_argument('--max_seq_length', type=int, default=1024)\n",
    "    parser.add_argument('--max_new_tokens', type=int, default=1)\n",
    "    parser.add_argument('--drop_duplicates', type=bool, default=False)\n",
    "    parser.add_argument('--cur_model_num', type=str, required=True)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    dist.init_process_group(\"nccl\")\n",
    "    world_size = dist.get_world_size()\n",
    "    local_rank = dist.get_rank()\n",
    "    print('local rank:', local_rank, torch.distributed.is_initialized(), world_size)\n",
    "    if local_rank == 0:\n",
    "        print(args)\n",
    "            \n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_dir,add_bos_token=True,trust_remote_code=True)\n",
    "    # left padding for batch inference\n",
    "    tokenizer.padding_side = 'left'\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    if args.input_file.endswith('.parquet'):\n",
    "        df = pd.read_parquet(args.input_file)[[\"Url\",\"UrlTitle\",\"UrlSnippet\",\"FullBody\"]]\n",
    "    elif args.input_file.endswith('.tsv'):\n",
    "        # args.input_file.endswith('.tsv') or args.input_file.endswith('.csv'):\n",
    "        if args.input_file.endswith('escape.tsv'):\n",
    "            df = pd.read_csv(args.input_file, sep=\"\\t\",header=None, names=[\"Url\",\"UrlTitle\",\"UrlSnippet\",\"FullBody\", \"Label\"]).drop_duplicates()\n",
    "            df = df[[\"Url\",\"UrlTitle\",\"UrlSnippet\",\"FullBody\"]]\n",
    "        elif args.input_file.endswith('Clean_60k.tsv'):\n",
    "            df = pd.read_csv(args.input_file, sep=\"\\t\",lineterminator=\"\\n\").drop_duplicates()\n",
    "        else:\n",
    "            df = pd.read_csv(args.input_file, sep='\\t')\n",
    "        df = df[[\"Url\",\"UrlTitle\",\"UrlSnippet\",\"FullBody\"]]\n",
    "    elif args.input_file.contains('KRsets'):\n",
    "        # original testset from Tyler\n",
    "        df = pd.read_csv(args.input_file, sep='\\t')\n",
    "        # df = df.rename(columns={'CrowdJudgment':'Label'})\n",
    "        df = df[[\"Url\",\"UrlTitle\",\"UrlSnippet\",\"FullBody\"]]\n",
    "    else:\n",
    "        # args.input_file.endswith('.csv')  # to predict zifan's training data\n",
    "        df = pd.read_csv(args.input_file, sep='\\t')\n",
    "        df = df[[\"Id\", \"Url\",\"UrlTitle\",\"UrlSnippet\",\"FullBody\"]]\n",
    "    \n",
    "    if world_size > 1:\n",
    "        df_rank = np.array_split(df, world_size)[local_rank]\n",
    "    else:\n",
    "        df_rank = df\n",
    "    # df_rank = df_rank.head(20) # For quick test\n",
    "    print(df.shape, df_rank.shape)    \n",
    "    ds = EvalDataset(df_rank, tokenizer, args.max_seq_length)\n",
    "    dataloader = DataLoader(ds, batch_size=args.batch_size, shuffle=False)\n",
    "\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # prediction\n",
    "    model = AutoModelForCausalLM.from_pretrained(args.load_from, \n",
    "                                        device_map=f'cuda:{local_rank}',\n",
    "                                        trust_remote_code=True, \n",
    "                                        quantization_config=quantization_config,\n",
    "                                        torch_dtype=torch.bfloat16,)\n",
    "    if world_size > 1:\n",
    "        ddp_model = DDP(model, device_ids=[local_rank])\n",
    "    model.eval()\n",
    "\n",
    "    ids = []\n",
    "    result_rank = []\n",
    "    prob_rank = []\n",
    "    \n",
    "    start = time.time()\n",
    "    for i, x in tqdm(enumerate(dataloader), total=len(dataloader), disable=(local_rank!=0)):\n",
    "        input_data = {key: value for key, value in x.items() if key in ['input_ids', 'attention_mask']}\n",
    "        model_inputs = BatchEncoding(input_data).to(f'cuda:{local_rank}')\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(**model_inputs, max_new_tokens=1,temperature=0.0,return_dict_in_generate=True, output_scores=True)\n",
    "            transition_scores = model.compute_transition_scores(output.sequences, output.scores, normalize_logits=True).to(\"cpu\")\n",
    "            generated_tokens = output.sequences.detach()[:, args.max_seq_length:].cpu()\n",
    "            score_length = transition_scores.shape[0]\n",
    "            result_rank.extend(tokenizer.batch_decode(generated_tokens, skip_special_tokens=True))\n",
    "            prob_rank.extend(np.exp(transition_scores.reshape(score_length).numpy()))\n",
    "            \n",
    "        if i % 100 == 0:\n",
    "            t = time.time() - start\n",
    "            n_samples = (i+1)*dataloader.batch_size\n",
    "            throughput = n_samples/t if t > 0 else 0\n",
    "            print(f'rank {local_rank}, total samples：{n_samples} throughput： {throughput:.2f} /s')\n",
    "        \n",
    "        # if i == 0 and local_rank == 0:\n",
    "        #     p = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "            # print(p)\n",
    "    \n",
    "    # print('rank', local_rank, 'inference done', len(result_rank))\n",
    "    # print(result_rank)\n",
    "    \n",
    "    # print('rank', local_rank, 'inference done', len(prob_rank))\n",
    "    # print(prob_rank)\n",
    "\n",
    "    df_rank['Prediction'] = result_rank\n",
    "    df_rank[\"Probability\"] = prob_rank\n",
    "        \n",
    "    if local_rank <= 0:\n",
    "        parent_dir = os.path.dirname(args.output_file)\n",
    "        if parent_dir and not os.path.exists(parent_dir):\n",
    "            os.makedirs(parent_dir, exist_ok=True)\n",
    "    \n",
    "    with open(args.output_file + f'_rank_{local_rank}.pkl', 'wb') as f:\n",
    "        pickle.dump(df_rank, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    if world_size > 1:\n",
    "        dist.barrier()\n",
    "        dfs = []\n",
    "        if local_rank <= 0:\n",
    "            # print('Run time ',time.time() - start)\n",
    "            # print('Merging results ...')\n",
    "            for i in range(world_size):\n",
    "                file_path = args.output_file + f'_rank_{i}.pkl'\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    dfs.append(pickle.load(f))\n",
    "                try:\n",
    "                    os.remove(file_path)\n",
    "                except OSError:\n",
    "                    pass\n",
    "            df_res = pd.concat(dfs, axis=0)\n",
    "    else:\n",
    "        df_res = df_rank\n",
    "    \n",
    "    if local_rank <= 0:\n",
    "        # print('total result:', df_res.shape)\n",
    "        output_file_name = args.output_file+'_'+args.cur_model_num+'.tsv'\n",
    "        df_res.to_csv(output_file_name, index=False, sep='\\t')\n",
    "        print('saved to:', output_file_name)\n",
    "\n",
    "    gc.collect()\n",
    "    ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
    "    torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "[2024-07-11 03:39:54,770] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-07-11 03:39:54,807] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-07-11 03:39:54,882] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:118: UserWarning: onnxruntime training package info: package_name: onnxruntime-training\n",
      "  warnings.warn(\"onnxruntime training package info: package_name: %s\" % package_name)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:119: UserWarning: onnxruntime training package info: __version__: 1.17.1\n",
      "  warnings.warn(\"onnxruntime training package info: __version__: %s\" % version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:120: UserWarning: onnxruntime training package info: cuda_version: 12.2\n",
      "  warnings.warn(\"onnxruntime training package info: cuda_version: %s\" % cuda_version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:121: UserWarning: onnxruntime build info: cudart_version: 12020\n",
      "  warnings.warn(\"onnxruntime build info: cudart_version: %s\" % cudart_version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:129: UserWarning: WARNING: failed to find cudart version that matches onnxruntime build info\n",
      "  warnings.warn(\"WARNING: failed to find cudart version that matches onnxruntime build info\")\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:130: UserWarning: WARNING: found cudart versions: [12010]\n",
      "  warnings.warn(\"WARNING: found cudart versions: %s\" % local_cudart_versions)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:118: UserWarning: onnxruntime training package info: package_name: onnxruntime-training\n",
      "  warnings.warn(\"onnxruntime training package info: package_name: %s\" % package_name)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:119: UserWarning: onnxruntime training package info: __version__: 1.17.1\n",
      "  warnings.warn(\"onnxruntime training package info: __version__: %s\" % version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:120: UserWarning: onnxruntime training package info: cuda_version: 12.2\n",
      "  warnings.warn(\"onnxruntime training package info: cuda_version: %s\" % cuda_version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:121: UserWarning: onnxruntime build info: cudart_version: 12020\n",
      "  warnings.warn(\"onnxruntime build info: cudart_version: %s\" % cudart_version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:129: UserWarning: WARNING: failed to find cudart version that matches onnxruntime build info\n",
      "  warnings.warn(\"WARNING: failed to find cudart version that matches onnxruntime build info\")\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:130: UserWarning: WARNING: found cudart versions: [12010]\n",
      "  warnings.warn(\"WARNING: found cudart versions: %s\" % local_cudart_versions)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:118: UserWarning: onnxruntime training package info: package_name: onnxruntime-training\n",
      "  warnings.warn(\"onnxruntime training package info: package_name: %s\" % package_name)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:119: UserWarning: onnxruntime training package info: __version__: 1.17.1\n",
      "  warnings.warn(\"onnxruntime training package info: __version__: %s\" % version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:120: UserWarning: onnxruntime training package info: cuda_version: 12.2\n",
      "  warnings.warn(\"onnxruntime training package info: cuda_version: %s\" % cuda_version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:121: UserWarning: onnxruntime build info: cudart_version: 12020\n",
      "  warnings.warn(\"onnxruntime build info: cudart_version: %s\" % cudart_version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:129: UserWarning: WARNING: failed to find cudart version that matches onnxruntime build info\n",
      "  warnings.warn(\"WARNING: failed to find cudart version that matches onnxruntime build info\")\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:130: UserWarning: WARNING: found cudart versions: [12010]\n",
      "  warnings.warn(\"WARNING: found cudart versions: %s\" % local_cudart_versions)\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "local rank: 2 True 3\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "local rank: 1 True 3\n",
      "[W Utils.hpp:133] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarInt)\n",
      "local rank: 0 True 3\n",
      "Namespace(load_from='/cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/Mixtral_Zifandata_v1/current_best_1800', tokenizer_dir='/cosmos/local/IndexQuality/FinetuneLLM/Mixtral-8x7B-Instruct-v0.1/', input_file='/cosmos/local/SpamLLM/Prod/KRsets/scrapekr1.2_UHRSValidation_withPlugin.tsv', output_file='/cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/Mixtral_Zifandata_v1/current_best_1800/scrapekr1.2_UHRSValidation_withPlugin.tsv', batch_size=12, max_new_tokens=1, drop_duplicates=False, cur_model_num='1800')\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "(2662, 4) (887, 4)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "(2662, 4) (887, 4)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "(2662, 4) (888, 4)\n",
      "NCCL version 2.19.4+cuda12.1\n",
      "\n",
      "node-0:2443212:2443634 [0] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:2443212:2443634 [0] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_0\n",
      "\n",
      "node-0:2443212:2443634 [0] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:2443212:2443634 [0] transport/net_ib.cc:193 NCCL WARN NET/IB : Unable to open device mlx5_0\n",
      "\n",
      "node-0:2443213:2443636 [1] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:2443213:2443636 [1] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_0\n",
      "\n",
      "node-0:2443213:2443636 [1] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:2443213:2443636 [1] transport/net_ib.cc:193 NCCL WARN NET/IB : Unable to open device mlx5_0\n",
      "\n",
      "node-0:2443214:2443639 [2] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:2443214:2443639 [2] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_0\n",
      "\n",
      "node-0:2443214:2443639 [2] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:2443214:2443639 [2] transport/net_ib.cc:193 NCCL WARN NET/IB : Unable to open device mlx5_0\n",
      "[rank0]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "[rank2]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "[rank1]:[W Utils.hpp:106] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function getCvarString)\n",
      "  0%|                                                    | 0/74 [00:00<?, ?it/s]/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "rank 1, total samples：12 throughput： 3.36 /s\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "rank 0, total samples：12 throughput： 3.32 /s\n",
      "  1%|▌                                           | 1/74 [00:03<04:23,  3.62s/it]rank 2, total samples：12 throughput： 3.28 /s\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  3%|█▏                                          | 2/74 [00:05<03:25,  2.86s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  4%|█▊                                          | 3/74 [00:08<03:06,  2.62s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  5%|██▍                                         | 4/74 [00:10<02:54,  2.49s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  7%|██▉                                         | 5/74 [00:12<02:46,  2.42s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  8%|███▌                                        | 6/74 [00:15<02:40,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  9%|████▏                                       | 7/74 [00:17<02:37,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 11%|████▊                                       | 8/74 [00:19<02:34,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 12%|█████▎                                      | 9/74 [00:22<02:31,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 14%|█████▊                                     | 10/74 [00:24<02:28,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 15%|██████▍                                    | 11/74 [00:26<02:25,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 16%|██████▉                                    | 12/74 [00:28<02:22,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 18%|███████▌                                   | 13/74 [00:31<02:20,  2.31s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 19%|████████▏                                  | 14/74 [00:33<02:18,  2.30s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 20%|████████▋                                  | 15/74 [00:35<02:17,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 22%|█████████▎                                 | 16/74 [00:38<02:14,  2.32s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 23%|█████████▉                                 | 17/74 [00:40<02:13,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 24%|██████████▍                                | 18/74 [00:42<02:10,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 26%|███████████                                | 19/74 [00:45<02:07,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 27%|███████████▌                               | 20/74 [00:47<02:06,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 28%|████████████▏                              | 21/74 [00:49<02:03,  2.34s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 30%|████████████▊                              | 22/74 [00:52<02:01,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 31%|█████████████▎                             | 23/74 [00:54<01:58,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 32%|█████████████▉                             | 24/74 [00:56<01:56,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 34%|██████████████▌                            | 25/74 [00:59<01:54,  2.33s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 35%|███████████████                            | 26/74 [01:01<01:53,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 36%|███████████████▋                           | 27/74 [01:03<01:50,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 38%|████████████████▎                          | 28/74 [01:06<01:48,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 39%|████████████████▊                          | 29/74 [01:08<01:46,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 41%|█████████████████▍                         | 30/74 [01:11<01:43,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 42%|██████████████████                         | 31/74 [01:13<01:41,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 43%|██████████████████▌                        | 32/74 [01:15<01:39,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 45%|███████████████████▏                       | 33/74 [01:18<01:36,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 46%|███████████████████▊                       | 34/74 [01:20<01:34,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 47%|████████████████████▎                      | 35/74 [01:22<01:32,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 49%|████████████████████▉                      | 36/74 [01:25<01:30,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 50%|█████████████████████▌                     | 37/74 [01:27<01:27,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 51%|██████████████████████                     | 38/74 [01:30<01:25,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 53%|██████████████████████▋                    | 39/74 [01:32<01:22,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 54%|███████████████████████▏                   | 40/74 [01:34<01:20,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 55%|███████████████████████▊                   | 41/74 [01:37<01:17,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 57%|████████████████████████▍                  | 42/74 [01:39<01:15,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 58%|████████████████████████▉                  | 43/74 [01:41<01:13,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 59%|█████████████████████████▌                 | 44/74 [01:44<01:11,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 61%|██████████████████████████▏                | 45/74 [01:46<01:08,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 62%|██████████████████████████▋                | 46/74 [01:48<01:06,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 64%|███████████████████████████▎               | 47/74 [01:51<01:03,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 65%|███████████████████████████▉               | 48/74 [01:53<01:01,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 66%|████████████████████████████▍              | 49/74 [01:56<00:59,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 68%|█████████████████████████████              | 50/74 [01:58<00:56,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 69%|█████████████████████████████▋             | 51/74 [02:00<00:54,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 70%|██████████████████████████████▏            | 52/74 [02:03<00:52,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 72%|██████████████████████████████▊            | 53/74 [02:05<00:49,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 73%|███████████████████████████████▍           | 54/74 [02:07<00:47,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 74%|███████████████████████████████▉           | 55/74 [02:10<00:45,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 76%|████████████████████████████████▌          | 56/74 [02:12<00:43,  2.41s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 77%|█████████████████████████████████          | 57/74 [02:15<00:40,  2.40s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 78%|█████████████████████████████████▋         | 58/74 [02:17<00:38,  2.39s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 80%|██████████████████████████████████▎        | 59/74 [02:19<00:35,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 81%|██████████████████████████████████▊        | 60/74 [02:22<00:33,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 82%|███████████████████████████████████▍       | 61/74 [02:24<00:30,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 84%|████████████████████████████████████       | 62/74 [02:27<00:28,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 85%|████████████████████████████████████▌      | 63/74 [02:29<00:26,  2.38s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 86%|█████████████████████████████████████▏     | 64/74 [02:31<00:23,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 88%|█████████████████████████████████████▊     | 65/74 [02:34<00:21,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 89%|██████████████████████████████████████▎    | 66/74 [02:36<00:18,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 91%|██████████████████████████████████████▉    | 67/74 [02:38<00:16,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 92%|███████████████████████████████████████▌   | 68/74 [02:41<00:14,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 93%|████████████████████████████████████████   | 69/74 [02:43<00:11,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 95%|████████████████████████████████████████▋  | 70/74 [02:45<00:09,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 96%|█████████████████████████████████████████▎ | 71/74 [02:48<00:07,  2.36s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 97%|█████████████████████████████████████████▊ | 72/74 [02:50<00:04,  2.35s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      " 99%|██████████████████████████████████████████▍| 73/74 [02:53<00:02,  2.37s/it]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "100%|███████████████████████████████████████████| 74/74 [02:55<00:00,  2.37s/it]\n",
      "saved to: /cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/Mixtral_Zifandata_v1/current_best_1800/scrapekr1.2_UHRSValidation_withPlugin.tsv_1800.tsv\n"
     ]
    }
   ],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=\"0,1,2\"\n",
    "!NCCL_DEBUG=WARN python -m torch.distributed.run  \\\n",
    "--nnodes 1 --nproc_per_node 3 inf_multi_mixtral_model_test.py --load_from /cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/Mixtral_Zifandata_v1/current_best_1800 \\\n",
    "--input_file \"/cosmos/local/SpamLLM/Prod/KRsets/scrapekr1.2_UHRSValidation_withPlugin.tsv\" \\\n",
    "--output_file \"/cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/Mixtral_Zifandata_v1/current_best_1800/scrapekr1.2_UHRSValidation_withPlugin.tsv\" \\\n",
    "--batch_size 12 \\\n",
    "--cur_model_num 1800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate AutoModelForClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98e7410a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing inf_v1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inf_v1.py\n",
    "import argparse\n",
    "import os\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import DataLoader, Dataset, DistributedSampler\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, AutoModelForCausalLM, default_data_collator, get_cosine_schedule_with_warmup, AutoConfig, BitsAndBytesConfig\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def get_df_rank(df):\n",
    "    if world_size > 1:\n",
    "        num_samples_keep = (len(df) // world_size ) * world_size\n",
    "        df = df.iloc[:num_samples_keep].copy()\n",
    "        df_rank = np.array_split(df, world_size)[local_rank]\n",
    "    else:\n",
    "        df_rank = df\n",
    "    return df_rank\n",
    "\n",
    "class EvalDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_seq_length):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.df.iloc[idx]\n",
    "        full_body = sample['FullBody']\n",
    "\n",
    "        text = ' '.join([str(sample['Url']), str(sample['UrlTitle']), str(sample['UrlSnippet']), str(sample['FullBody'])])\n",
    "        label = int(sample['Label'])\n",
    "        \n",
    "        res = self.tokenizer(text, max_length=self.max_seq_length-1, return_attention_mask=False, padding=False, truncation=True)\n",
    "        res['input_ids'] = res['input_ids'] + [self.tokenizer.eos_token_id]\n",
    "        res = self.tokenizer.pad(res, max_length=self.max_seq_length, padding='max_length', return_attention_mask=True, return_tensors='pt')\n",
    "        \n",
    "        return {\n",
    "            'input_ids': res['input_ids'],  # shape: torch.Size([bs, 1024])\n",
    "            'attention_mask': res['attention_mask'],    # shape: torch.Size([bs, 1024])\n",
    "            'labels': torch.tensor(label),   # shape: torch.Size([bs])\n",
    "            # 'ids': torch.tensor(int(sample['Id']))\n",
    "        }\n",
    "\n",
    "if __name__ == '__main__':    \n",
    "    parser = argparse.ArgumentParser()\n",
    "    # parser.add_argument('--load_from', type=str, default='intfloat/e5-mistral-7b-instruct')\n",
    "    parser.add_argument('--load_from', type=str, default='Qwen/Qwen2.5-0.5B-Instruct')\n",
    "    parser.add_argument('--model_path', type=str)\n",
    "    parser.add_argument('--eval_set', type=str)\n",
    "    parser.add_argument('--max_seq_length', type=int, default=1024)\n",
    "    parser.add_argument('--batch_size', type=int, default=128)\n",
    "    parser.add_argument('--is_output_embedding', type=bool, default=False)\n",
    "    parser.add_argument('--output_dir', type=str)\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    dist.init_process_group(\"nccl\")\n",
    "    world_size = dist.get_world_size()\n",
    "    local_rank = dist.get_rank()\n",
    "    print('local rank:', local_rank, torch.distributed.is_initialized(), world_size)\n",
    "    if local_rank == 0:\n",
    "        print(args)\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(args.load_from, load_in_8bit=False, device_map=f'cuda:{local_rank}', torch_dtype=torch.float16)\n",
    "    model_state_dict = torch.load(args.model_path)\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    model.config.pad_token_id = model.config.eos_token_id  # new \n",
    "    if world_size > 1:\n",
    "        ddp_model = DDP(model, device_ids=[local_rank])\n",
    "    model.eval() \n",
    "    \n",
    "    if 'scrapekr' in args.eval_set:\n",
    "        df = pd.read_parquet(args.eval_set)[[\"Url\",\"UrlTitle\",\"UrlSnippet\",\"FullBody\",'UrlExpectedLabel']]\n",
    "        # df.rename(columns={'CrowdJudgment':'Label'}, inplace=True)\n",
    "        mapping = {'detrimental spam':1, 'non-detrimental spam':1, 'not spam':0}\n",
    "        df['UrlExpectedLabel'] = df['UrlExpectedLabel'].map(mapping)\n",
    "        df.rename(columns={'UrlExpectedLabel':'Label'}, inplace=True)\n",
    "    elif 'test_dataset_2024_03_05' in args.eval_set:\n",
    "        df = pd.read_csv(args.eval_set,sep=\"\\t\")[[\"Url\",\"UrlTitle\",\"UrlSnippet\",\"FullBody\",\"Label\"]]\n",
    "        df[\"Label\"] = df[\"Label\"].apply(lambda x:int(x.replace(\"<ANS>\",\"\").replace(\"</ANS>\",\"\")))\n",
    "    elif 'spamgtx5.0' in args.eval_set:\n",
    "        df = pd.read_parquet(args.eval_set)\n",
    "        df[\"Label\"] = df[\"AuditorJudgment\"].apply(lambda x: 0 if x =='not spam' else 1)\n",
    "        df = df[[\"Url\",\"UrlTitle\",\"UrlSnippet\",\"FullBody\",\"Label\"]]\n",
    "    elif 'Clean_60k' in args.eval_set:\n",
    "        df = pd.read_csv(args.eval_set,sep=\"\\t\",lineterminator=\"\\n\").drop_duplicates()  # columns: ['Unnamed: 0', 'Url', 'UrlTitle', 'UrlSnippet', 'FullBody', 'Label']\n",
    "    elif '8k_with_flipped_labels' in args.eval_set:\n",
    "        df = pd.read_csv(args.eval_set,sep=\"\\t\")\n",
    "    elif 'SpamLLM_Output_2.6.0_v4_with_renamed_schema' in args.eval_set:\n",
    "        df = pd.read_csv(args.eval_set, sep='\\t')\n",
    "        df = df[[\"Id\", \"Url\",\"UrlTitle\",\"UrlSnippet\",\"FullBody\",\"Label\"]]\n",
    "        df = df[df['Label'].notna()]\n",
    "    elif 'auditor' in args.eval_set:\n",
    "        df = pd.read_csv(args.eval_set, sep=\"\\t\",header=None, names=[\"Url\",\"UrlTitle\",\"UrlSnippet\",\"FullBody\", \"Label\"]) \n",
    "        df = df[df['Label'].notna()]\n",
    "\n",
    "    doc_df_rank = get_df_rank(df)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.load_from)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    eval_dataset = EvalDataset(doc_df_rank, tokenizer, args.max_seq_length)\n",
    "    eval_dataloader = DataLoader(eval_dataset, batch_size=args.batch_size, shuffle=False)\n",
    "    \n",
    "    eval_data = []\n",
    "    with open(args.output_dir+'_'+str(local_rank)+\".tsv\", 'w') as f:\n",
    "        for cur, x in tqdm(enumerate(eval_dataloader), total=len(eval_dataloader), disable=(local_rank!=0)):\n",
    "            model_input = BatchEncoding(x).to(f'cuda:{local_rank}')\n",
    "            with torch.no_grad():\n",
    "                label = model_input[\"labels\"]\n",
    "                # id = model_input['ids']\n",
    "                outputs = model(input_ids=model_input[\"input_ids\"], attention_mask=model_input[\"attention_mask\"])\n",
    "                if is_output_embedding:\n",
    "                    param_generator = model.named_parameters()\n",
    "                    all_params = list(param_generator)\n",
    "                    embedding_name = all_params[-2][0]\n",
    "                    embedding = all_params[-2][1]\n",
    "                    embedding_str = '|'.join(map(str, embedding.tolist()))\n",
    "                output_logits = outputs.logits\n",
    "                output_prob = F.softmax(output_logits, dim=1)\n",
    "                output_prob = output_prob[:, 1]\n",
    "                for i in range(len(label)):\n",
    "                    # f.write(\"{0}\\t{1}\\t{2}\\n\".format(id[i].item(), label[i].item(), output_prob[i].item()))\n",
    "                    f.write(\"{0}\\t{1}\\n\".format(label[i].item(), output_prob[i].item()))\n",
    "    \n",
    "    print(f'the prediction result is saved here: {args.output_dir}_{str(local_rank)}.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E5 Inference Command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "[W1011 00:18:17.374351710 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())\n",
      "local rank: 1 True 3\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[W1011 00:18:17.396745584 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())\n",
      "local rank: 0 True 3\n",
      "Namespace(load_from='intfloat/e5-mistral-7b-instruct', model_path='/cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/Mistral_New_ym_e5_v1/current_best_1000/pytorch_model.bin', eval_set='/cosmos/local/users/zifanwang/SpamLLM/data/auditor_ym_escape1.tsv', max_seq_length=1024, batch_size=32, output_dir='/cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/Mistral_New_ym_e5_v1/current_best_1000/auditor_ym_escape1.tsv')\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[W1011 00:18:17.429900033 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())\n",
      "local rank: 2 True 3\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:03<00:00,  1.58s/it]\n",
      "Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at intfloat/e5-mistral-7b-instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/cosmos/local/IndexQuality/ContentModels/DataAugmentation/data/CBSpam_v3/Code/GeneralCodeRepo/Project/inf_qwen_v1.py:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(args.model_path)\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:03<00:00,  1.57s/it]\n",
      "Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at intfloat/e5-mistral-7b-instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/cosmos/local/IndexQuality/ContentModels/DataAugmentation/data/CBSpam_v3/Code/GeneralCodeRepo/Project/inf_qwen_v1.py:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(args.model_path)\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:03<00:00,  1.59s/it]\n",
      "Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at intfloat/e5-mistral-7b-instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/cosmos/local/IndexQuality/ContentModels/DataAugmentation/data/CBSpam_v3/Code/GeneralCodeRepo/Project/inf_qwen_v1.py:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(args.model_path)\n",
      "NCCL version 2.19.4+cuda12.4\n",
      "\n",
      "node-0:280794:280879 [0] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:280794:280879 [0] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_0\n",
      "\n",
      "node-0:280794:280879 [0] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:280794:280879 [0] transport/net_ib.cc:193 NCCL WARN NET/IB : Unable to open device mlx5_0\n",
      "\n",
      "node-0:280795:280881 [1] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:280795:280881 [1] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_0\n",
      "\n",
      "node-0:280795:280881 [1] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:280795:280881 [1] transport/net_ib.cc:193 NCCL WARN NET/IB : Unable to open device mlx5_0\n",
      "\n",
      "node-0:280796:280883 [2] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:280796:280883 [2] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_0\n",
      "\n",
      "node-0:280796:280883 [2] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:280796:280883 [2] transport/net_ib.cc:193 NCCL WARN NET/IB : Unable to open device mlx5_0\n",
      "[rank0]:[W1011 00:18:46.506628996 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())\n",
      "[rank1]:[W1011 00:18:46.506799873 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())\n",
      "[rank2]:[W1011 00:18:46.507137734 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())\n",
      "[2024-10-11 00:18:46,447] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-10-11 00:18:46,458] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-10-11 00:18:46,469] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:118: UserWarning: onnxruntime training package info: package_name: onnxruntime-training\n",
      "  warnings.warn(\"onnxruntime training package info: package_name: %s\" % package_name)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:119: UserWarning: onnxruntime training package info: __version__: 1.18.0\n",
      "  warnings.warn(\"onnxruntime training package info: __version__: %s\" % version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:120: UserWarning: onnxruntime training package info: cuda_version: 12.2\n",
      "  warnings.warn(\"onnxruntime training package info: cuda_version: %s\" % cuda_version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:121: UserWarning: onnxruntime build info: cudart_version: 12020\n",
      "  warnings.warn(\"onnxruntime build info: cudart_version: %s\" % cudart_version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:129: UserWarning: WARNING: failed to find cudart version that matches onnxruntime build info\n",
      "  warnings.warn(\"WARNING: failed to find cudart version that matches onnxruntime build info\")\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:130: UserWarning: WARNING: found cudart versions: [12040]\n",
      "  warnings.warn(\"WARNING: found cudart versions: %s\" % local_cudart_versions)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:118: UserWarning: onnxruntime training package info: package_name: onnxruntime-training\n",
      "  warnings.warn(\"onnxruntime training package info: package_name: %s\" % package_name)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:119: UserWarning: onnxruntime training package info: __version__: 1.18.0\n",
      "  warnings.warn(\"onnxruntime training package info: __version__: %s\" % version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:120: UserWarning: onnxruntime training package info: cuda_version: 12.2\n",
      "  warnings.warn(\"onnxruntime training package info: cuda_version: %s\" % cuda_version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:121: UserWarning: onnxruntime build info: cudart_version: 12020\n",
      "  warnings.warn(\"onnxruntime build info: cudart_version: %s\" % cudart_version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:129: UserWarning: WARNING: failed to find cudart version that matches onnxruntime build info\n",
      "  warnings.warn(\"WARNING: failed to find cudart version that matches onnxruntime build info\")\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:130: UserWarning: WARNING: found cudart versions: [12040]\n",
      "  warnings.warn(\"WARNING: found cudart versions: %s\" % local_cudart_versions)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:118: UserWarning: onnxruntime training package info: package_name: onnxruntime-training\n",
      "  warnings.warn(\"onnxruntime training package info: package_name: %s\" % package_name)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:119: UserWarning: onnxruntime training package info: __version__: 1.18.0\n",
      "  warnings.warn(\"onnxruntime training package info: __version__: %s\" % version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:120: UserWarning: onnxruntime training package info: cuda_version: 12.2\n",
      "  warnings.warn(\"onnxruntime training package info: cuda_version: %s\" % cuda_version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:121: UserWarning: onnxruntime build info: cudart_version: 12020\n",
      "  warnings.warn(\"onnxruntime build info: cudart_version: %s\" % cudart_version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:129: UserWarning: WARNING: failed to find cudart version that matches onnxruntime build info\n",
      "  warnings.warn(\"WARNING: failed to find cudart version that matches onnxruntime build info\")\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:130: UserWarning: WARNING: found cudart versions: [12040]\n",
      "  warnings.warn(\"WARNING: found cudart versions: %s\" % local_cudart_versions)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  0%|                                                   | 0/457 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 52%|█████████████████████▎                   | 237/457 [23:08<21:38,  5.90s/it]the prediction result is saved here: /cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/Mistral_New_ym_e5_v1/current_best_1000/auditor_ym_escape1.tsv_2.tsv\n",
      " 52%|█████████████████████▍                   | 239/457 [23:20<21:22,  5.89s/it]the prediction result is saved here: /cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/Mistral_New_ym_e5_v1/current_best_1000/auditor_ym_escape1.tsv_1.tsv\n",
      "100%|█████████████████████████████████████████| 457/457 [44:38<00:00,  5.86s/it]\n",
      "the prediction result is saved here: /cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/Mistral_New_ym_e5_v1/current_best_1000/auditor_ym_escape1.tsv_0.tsv\n"
     ]
    }
   ],
   "source": [
    "# !export CUDA_VISIBLE_DEVICES=\"0,1,2\"\n",
    "# !NCCL_DEBUG=WARN python -m torch.distributed.run  \\\n",
    "# --master_port 29501 --nnodes 1 --nproc_per_node 3 inf_multi_e5_model_for_predict_zifan_data6.py \\\n",
    "# --model_path /cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/Mixtral_New_ym_e5/current_best_1200/pytorch_model.bin \\\n",
    "# --eval_set /cosmos/local/IndexQuality/FinetuneLLM/TrainingData/SpamLLM_Output_2.6.0_v4_with_renamed_schema.csv \\\n",
    "# --max_seq_length 1024 \\\n",
    "# --batch_size 32 \\\n",
    "# --output_dir /cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/Mixtral_New_ym_e5/current_best_1200/zifan_data\n",
    "\n",
    "!export CUDA_VISIBLE_DEVICES=\"1,2,3\"\n",
    "!NCCL_DEBUG=WARN python -m torch.distributed.run  \\\n",
    "--master_port 29501 --nnodes 1 --nproc_per_node 3 inf_qwen_v1.py \\\n",
    "--load_from 'intfloat/e5-mistral-7b-instruct' \\\n",
    "--model_path /cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/Mistral_New_ym_e5_v1/current_best_1000/pytorch_model.bin \\\n",
    "--eval_set /cosmos/local/users/zifanwang/SpamLLM/data/auditor_ym_escape1.tsv \\\n",
    "--max_seq_length 1024 \\\n",
    "--batch_size 32 \\\n",
    "--output_dir /cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/Mistral_New_ym_e5_v1/current_best_1000/auditor_ym_escape1.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qwen Inference Command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "[W1010 19:59:58.703296009 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())\n",
      "local rank: 1 True 3\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[W1010 19:59:58.750110060 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())\n",
      "local rank: 0 True 3\n",
      "Namespace(load_from='Qwen/Qwen2.5-0.5B-Instruct', model_path='/cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/qwen_no_overlap_o1_a3_v1/model_1200/pytorch_model.bin', eval_set='/cosmos/local/IndexQuality/FinetuneLLM/EvaluationSets/scrapekr1.2_spamllm2.4.parquet', max_seq_length=1024, batch_size=32, output_dir='/cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/qwen_no_overlap_o1_a3_v1/model_1200/model_1200')\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[W1010 19:59:58.834560666 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())\n",
      "local rank: 2 True 3\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen2.5-0.5B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/cosmos/local/IndexQuality/ContentModels/DataAugmentation/data/CBSpam_v3/Code/GeneralCodeRepo/Project/inf_qwen_v1.py:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(args.model_path)\n",
      "Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen2.5-0.5B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/cosmos/local/IndexQuality/ContentModels/DataAugmentation/data/CBSpam_v3/Code/GeneralCodeRepo/Project/inf_qwen_v1.py:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(args.model_path)\n",
      "Some weights of Qwen2ForSequenceClassification were not initialized from the model checkpoint at Qwen/Qwen2.5-0.5B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/cosmos/local/IndexQuality/ContentModels/DataAugmentation/data/CBSpam_v3/Code/GeneralCodeRepo/Project/inf_qwen_v1.py:71: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state_dict = torch.load(args.model_path)\n",
      "NCCL version 2.19.4+cuda12.4\n",
      "\n",
      "node-0:150107:150145 [0] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:150107:150145 [0] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_0\n",
      "\n",
      "node-0:150107:150145 [0] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:150107:150145 [0] transport/net_ib.cc:193 NCCL WARN NET/IB : Unable to open device mlx5_0\n",
      "\n",
      "node-0:150109:150147 [2] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:150109:150147 [2] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_0\n",
      "\n",
      "node-0:150109:150147 [2] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:150109:150147 [2] transport/net_ib.cc:193 NCCL WARN NET/IB : Unable to open device mlx5_0\n",
      "\n",
      "node-0:150108:150149 [1] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:150108:150149 [1] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_0\n",
      "\n",
      "node-0:150108:150149 [1] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:150108:150149 [1] transport/net_ib.cc:193 NCCL WARN NET/IB : Unable to open device mlx5_0\n",
      "[rank1]:[W1010 20:00:02.808343720 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())\n",
      "[rank0]:[W1010 20:00:02.808377804 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())\n",
      "[rank2]:[W1010 20:00:02.808477542 Utils.hpp:110] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())\n",
      "[2024-10-10 20:00:02,684] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-10-10 20:00:02,704] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-10-10 20:00:02,707] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:118: UserWarning: onnxruntime training package info: package_name: onnxruntime-training\n",
      "  warnings.warn(\"onnxruntime training package info: package_name: %s\" % package_name)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:119: UserWarning: onnxruntime training package info: __version__: 1.18.0\n",
      "  warnings.warn(\"onnxruntime training package info: __version__: %s\" % version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:120: UserWarning: onnxruntime training package info: cuda_version: 12.2\n",
      "  warnings.warn(\"onnxruntime training package info: cuda_version: %s\" % cuda_version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:121: UserWarning: onnxruntime build info: cudart_version: 12020\n",
      "  warnings.warn(\"onnxruntime build info: cudart_version: %s\" % cudart_version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:129: UserWarning: WARNING: failed to find cudart version that matches onnxruntime build info\n",
      "  warnings.warn(\"WARNING: failed to find cudart version that matches onnxruntime build info\")\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:130: UserWarning: WARNING: found cudart versions: [12040]\n",
      "  warnings.warn(\"WARNING: found cudart versions: %s\" % local_cudart_versions)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:118: UserWarning: onnxruntime training package info: package_name: onnxruntime-training\n",
      "  warnings.warn(\"onnxruntime training package info: package_name: %s\" % package_name)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:119: UserWarning: onnxruntime training package info: __version__: 1.18.0\n",
      "  warnings.warn(\"onnxruntime training package info: __version__: %s\" % version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:120: UserWarning: onnxruntime training package info: cuda_version: 12.2\n",
      "  warnings.warn(\"onnxruntime training package info: cuda_version: %s\" % cuda_version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:121: UserWarning: onnxruntime build info: cudart_version: 12020\n",
      "  warnings.warn(\"onnxruntime build info: cudart_version: %s\" % cudart_version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:129: UserWarning: WARNING: failed to find cudart version that matches onnxruntime build info\n",
      "  warnings.warn(\"WARNING: failed to find cudart version that matches onnxruntime build info\")\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:130: UserWarning: WARNING: found cudart versions: [12040]\n",
      "  warnings.warn(\"WARNING: found cudart versions: %s\" % local_cudart_versions)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:118: UserWarning: onnxruntime training package info: package_name: onnxruntime-training\n",
      "  warnings.warn(\"onnxruntime training package info: package_name: %s\" % package_name)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:119: UserWarning: onnxruntime training package info: __version__: 1.18.0\n",
      "  warnings.warn(\"onnxruntime training package info: __version__: %s\" % version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:120: UserWarning: onnxruntime training package info: cuda_version: 12.2\n",
      "  warnings.warn(\"onnxruntime training package info: cuda_version: %s\" % cuda_version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:121: UserWarning: onnxruntime build info: cudart_version: 12020\n",
      "  warnings.warn(\"onnxruntime build info: cudart_version: %s\" % cudart_version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:129: UserWarning: WARNING: failed to find cudart version that matches onnxruntime build info\n",
      "  warnings.warn(\"WARNING: failed to find cudart version that matches onnxruntime build info\")\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:130: UserWarning: WARNING: found cudart versions: [12040]\n",
      "  warnings.warn(\"WARNING: found cudart versions: %s\" % local_cudart_versions)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/numpy/core/fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  0%|                                                    | 0/28 [00:00<?, ?it/s]You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a Qwen2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 46%|███████████████████▉                       | 13/28 [00:10<00:12,  1.22it/s]the prediction result is saved here: /cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/qwen_no_overlap_o1_a3_v1/model_1200/model_1200_2.tsv\n",
      "the prediction result is saved here: /cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/qwen_no_overlap_o1_a3_v1/model_1200/model_1200_1.tsv\n",
      "100%|███████████████████████████████████████████| 28/28 [00:20<00:00,  1.38it/s]\n",
      "the prediction result is saved here: /cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/qwen_no_overlap_o1_a3_v1/model_1200/model_1200_0.tsv\n"
     ]
    }
   ],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=\"1,2,3\"\n",
    "!NCCL_DEBUG=WARN python -m torch.distributed.run  \\\n",
    "--master_port 29501 --nnodes 1 --nproc_per_node 3 inf_qwen_v1.py \\\n",
    "--model_path /cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/qwen_no_overlap_o1_a3_v1/model_1200/pytorch_model.bin \\\n",
    "--load_from Qwen/Qwen2.5-0.5B-Instruct \\\n",
    "--eval_set /cosmos/local/IndexQuality/FinetuneLLM/EvaluationSets/scrapekr1.2_spamllm2.4.parquet \\\n",
    "--max_seq_length 1024 \\\n",
    "--batch_size 32 \\\n",
    "--is_output_embedding True \\\n",
    "--output_dir /cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/qwen_no_overlap_o1_a3_v1/model_1200/model_1200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute all E5 models metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total length: 2661\n",
      "Threshold:  0.77392578125\n",
      "Fbeta:  0.6760204081534599\n",
      "auprc:  0.6982521300535821\n",
      "Precision:  0.7386759581881533\n",
      "Recall:  0.5047619047619047\n"
     ]
    }
   ],
   "source": [
    "Label = []\n",
    "Pred = []\n",
    "for i in range(0,3,1):\n",
    "    with open(f'/cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/Mixtral_New_ym_e5/model_100/eval_detail_{i}.tsv', 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            split_line = line.strip().split('\\t')\n",
    "            Label.append(float(split_line[0]))\n",
    "            Pred.append(float(split_line[1]))\n",
    "print(f'total length: {len(Label)}')\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import numpy as np\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(Label, Pred)\n",
    "beta=0.5\n",
    "fbeta = (1 + beta**2) * precision * recall / ((beta**2) * precision + recall + 0.00000000001)\n",
    "index = np.nanargmax(fbeta)\n",
    "\n",
    "print(\"Threshold: \",thresholds[index])\n",
    "print(\"Fbeta: \",fbeta[index])\n",
    "print(\"auprc: \", average_precision_score(Label, Pred))\n",
    "print(\"Precision: \",precision[index])\n",
    "print(\"Recall: \",recall[index]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
