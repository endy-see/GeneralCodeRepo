{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train_script_optimized_e5_8.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_script_optimized_e5_8.py\n",
    "# %%writefile train_script_optimized_e5_2.py\n",
    "# %%writefile train_script_optimized_e5_3.py\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "import shutil\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DistributedSampler,Dataset, DataLoader\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import MixedPrecision\n",
    "from transformers.models.mixtral.modeling_mixtral import MixtralDecoderLayer\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, AutoModelForCausalLM, default_data_collator, get_cosine_schedule_with_warmup, AutoConfig, BitsAndBytesConfig\n",
    "import gc\n",
    "from transformers import MistralForCausalLM\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import subprocess\n",
    "import utils\n",
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class TrainingDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_seq_length):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.df.iloc[idx]\n",
    "        full_body = sample['FullBody']\n",
    "   \n",
    "        text = ' '.join([str(sample['Url']), str(sample['UrlTitle']), str(sample['UrlSnippet']), str(sample['FullBody'])])\n",
    "        label = int(sample['Label'])\n",
    "        \n",
    "        res = self.tokenizer(text, max_length=self.max_seq_length-1, return_attention_mask=False, padding=False, truncation=True)\n",
    "        res['input_ids'] = res['input_ids'] + [self.tokenizer.eos_token_id]\n",
    "        res = self.tokenizer.pad(res, max_length=self.max_seq_length, padding='max_length', return_attention_mask=True, return_tensors='pt')\n",
    "        \n",
    "        return {\n",
    "            'input_ids': res['input_ids'],  # shape: torch.Size([bs, 1024])\n",
    "            'attention_mask': res['attention_mask'],    # shape: torch.Size([bs, 1024])\n",
    "            'labels': torch.tensor(label)   # shape: torch.Size([bs])\n",
    "        }\n",
    "\n",
    "def setup(args):\n",
    "    # setup distributed environment\n",
    "    world_size = int(os.environ[\"WORLD_SIZE\"])  # 参与训练的总进程数，即GPU数量。在分布式训练中，每个GPU对应一个进程。因此，world_size是你在分布式训练中使用的GPU数量\n",
    "    rank = int(os.environ[\"RANK\"])      # rank是每个进程的唯一标识符，用于区分不同的进程。在分布式训练中，每个进程都有一个独特的rank，从0到world_size-1\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])  # local_rank:是在单个节点内部的进程的本地标识符。如果你在多个节点上运行分布式训练，每个节点都有自己的local_rank。\n",
    "    # 比如，你在2台服务器上运行分布式训练，每台服务器有4个GPU。那么总的world_size是8，进程的rank分别是[0,1,2,3,4,5,6,7]。在每台服务器上local_rank分别是[0,1,2,3]\n",
    "    print(f\"World size: {world_size}, rank: {rank}, local rank: {local_rank}\")\n",
    "\n",
    "    timeout = timedelta(hours=5) # 时间间隔对象，表示在初始化分布式进程组时等待的最大时间。这里表示如果在5小时内无法成功初始化分布式进程组，将引发超时错误\n",
    "    dist.init_process_group(\"nccl\", timeout=timeout, rank=rank, world_size=world_size) # 初始化分布式进程组，nccl表示用用NVIDIA Collective Communications Library作为后端，rank表示当前进程的标识符，world_size表示总进程数\n",
    "    assert torch.distributed.is_initialized()\n",
    "\n",
    "    torch.cuda.set_device(local_rank)  # 设置当前进程使用的GPU设备，local_rank表示当前节点内部的进程的本地标识符\n",
    "    torch.cuda.empty_cache()    # 用于清空GPU缓存，以释放内存\n",
    "\n",
    "    if is_master(rank): # 只有当前是主进程时才会执行打印参数的操作\n",
    "        print(args)\n",
    "    \n",
    "    # setup tokenizer and dataloader\n",
    "    \n",
    "    # if args.model == 'llama2':\n",
    "    #     tokenizer = LlamaTokenizer.from_pretrained(args.load_from)\n",
    "    # elif args.model == 'mistral':\n",
    "    #     tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "    # elif args.model == 'turing':\n",
    "    #     # tokenizer = TNLGv4Tokenizer()\n",
    "    #     tokenizer = GPT2TokenizerFast.from_pretrained('Xenova/gpt-4')\n",
    "    # elif args.model == 'mixtral':\n",
    "    #     tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\")\n",
    "    # else:\n",
    "    #     print(f\"model {args.model} not supported\")\n",
    "    #     raise NotImplementedError\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.load_from)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # setup model and fsdp\n",
    "    if args.model == 'llama2':\n",
    "        model = LlamaForCausalLM.from_pretrained(args.load_from, load_in_8bit=False, device_map=None, torch_dtype=torch.bfloat16, use_cache=True)\n",
    "        if args.neftune_alpha is not None:\n",
    "            print('NEFTUNE enabled')\n",
    "            # Save the old forward function as a class attribute\n",
    "            torch.nn.Embedding.old_forward = model.model.embed_tokens.forward\n",
    "\n",
    "            # Define the new forward function\n",
    "            def new_forward(self, x):\n",
    "                # Call the old forward function and get its output\n",
    "                #print('neftune forward')\n",
    "                out = self.old_forward(x)\n",
    "                dims = torch.tensor(out.size(1) * out.size(2))\n",
    "                mag_norm = args.neftune_alpha / torch.sqrt(dims)\n",
    "                return out + torch.zeros_like(out).uniform_(-mag_norm, mag_norm)\n",
    "\n",
    "            # Replace the forward function of the embedding object with the new one\n",
    "            model.model.embed_tokens.forward = new_forward.__get__(model.model.embed_tokens, torch.nn.Embedding)\n",
    "    elif args.model == \"mistral\":\n",
    "        # e5\n",
    "        # model = MistralForCausalLM.from_pretrained(args.load_from, load_in_8bit=False, device_map=None, torch_dtype=torch.float16, use_cache=True)\n",
    "        # model = AutoModel.from_pretrained(args.load_from, load_in_8bit=False, device_map=None, torch_dtype=torch.float16)  # device_map=f'cuda:{local_rank}'\n",
    "        # model = AutoModel.from_pretrained(args.load_from, load_in_8bit=False, device_map=f'cuda:{local_rank}', torch_dtype=torch.float16) \n",
    "        model = AutoModelForSequenceClassification.from_pretrained(args.load_from, num_labels=2, load_in_8bit=False, device_map=f'cuda:{local_rank}', torch_dtype=torch.float16) # zhym\n",
    "        # model_state_dict = torch.load(args.model_path)\n",
    "        # model.load_state_dict(model_state_dict)\n",
    "    elif args.model == \"mixtral\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(args.load_from,\n",
    "                                                    load_in_8bit=False, # 让模型不要以8位精度加载，使用默认精度加载权重\n",
    "                                                    device_map=None,    # 没有自定义的设备映射，此时如果你在多个GPU上运行，模型的参数将均匀分配到所有可用的GPU上，适用于参数量大的模型\n",
    "                                                    torch_dtype=torch.bfloat16,  # 用16位浮点数作为权重数据类型\n",
    "                                                    use_cache=True  # 模型将使用缓存来加速计算\n",
    "                                                    )\n",
    "    else:\n",
    "        print(f\"model {args.model} not supported\")\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    if is_master(rank):\n",
    "        print(f\"model config: {model.config}\")\n",
    "        print(model)\n",
    "            \n",
    "    model.to(dtype=torch.bfloat16)\n",
    "\n",
    "    #utils.apply_fsdp_checkpointing(model) if args.model == 'llama2' else model.gradient_checkpointing_enable() # for turing\n",
    "    # model.gradient_checkpointing_enable()\n",
    "    #print(f\"Gradient Checkpointing: {model.is_gradient_checkpointing}\")\n",
    "\n",
    "    model = FSDP(\n",
    "        model,\n",
    "        auto_wrap_policy=utils.get_mistral_wrapper(),\n",
    "        #mixed_precision=mixed_precision_policy,\n",
    "        mixed_precision=None,\n",
    "        sharding_strategy=utils.fsdp_config.sharding_strategy,\n",
    "        device_id=torch.cuda.current_device(),\n",
    "        limit_all_gathers=True,\n",
    "        sync_module_states=False,\n",
    "        param_init_fn=None\n",
    "    )\n",
    "\n",
    "    # Mistral-E5\n",
    "    utils.apply_fsdp_checkpointing(model) \n",
    "    \n",
    "    # setup optimizer\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=args.lr,\n",
    "        weight_decay=0.0,\n",
    "    )\n",
    "    \n",
    "    # df = pd.read_csv(\"/cosmos/local/IndexQuality/FinetuneLLM/TrainingData/O1_A3_crowd_training_data_1.tsv\",sep=\"\\t\")\n",
    "    df = pd.read_csv(args.training_data_from, sep='\\t', usecols=args.columns)\n",
    "    df = df[df['Label'].notna()]\n",
    "    print(f'training dataset length: {len(df)}')\n",
    "    print(f'training data columns: {df.columns}')\n",
    "    print(f'training data pos/neg counts: {df.Label.value_counts()}')\n",
    "    \n",
    "    train_ds =  TrainingDataset(df, tokenizer, args.max_seq_length)\n",
    "    \n",
    "    train_sampler = DistributedSampler(\n",
    "        train_ds,\n",
    "        rank=rank,\n",
    "        num_replicas=world_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.num_workers,  # 4\n",
    "        pin_memory=True,\n",
    "        sampler=train_sampler,\n",
    "        drop_last=True,\n",
    "        collate_fn=default_data_collator,\n",
    "    )\n",
    "\n",
    "    total_iteration = args.num_epochs * (len(train_dataloader)/(args.batch_size*args.gpu_counts))\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=len(train_dataloader) * args.warmup,\n",
    "        num_training_steps=len(train_dataloader) * args.num_epochs\n",
    "        # num_warmup_steps=total_iteration*0.1,\n",
    "        # num_training_steps=total_iteration\n",
    "    )\n",
    "\n",
    "    return model, train_dataloader, optimizer, scheduler, local_rank, rank, world_size, tokenizer\n",
    "\n",
    "\n",
    "def is_master(rank):\n",
    "    # In a multi-node setup, the master process is rank 0\n",
    "    return rank == 0\n",
    "        \n",
    "class RunningMean(object):\n",
    "    def __init__(self, local_rank, N=100):\n",
    "        self.N = N\n",
    "        self.local_rank = local_rank\n",
    "        self.data = []\n",
    "\n",
    "    def add(self, x):\n",
    "        self.data.append(x)\n",
    "\n",
    "    def mean_all_rank(self):\n",
    "        d = self.data[-self.N:]\n",
    "        if len(d) > 0:\n",
    "            m = sum(d) / len(d)\n",
    "        else:\n",
    "            m = 0\n",
    "        t = torch.tensor([m]).to(self.local_rank) # 将m转换为PyTorch张量，并将其移到self.local_rank对应的设备上\n",
    "        all_t = torch.zeros(dist.get_world_size(), dtype=t.dtype).to(self.local_rank)   # 创建一个全零张量all_t，大小为分布式环境中的进程数\n",
    "        dist.all_gather_into_tensor(all_t, t)   # 将所有进程的m收集到all_t中，并计算所有进程的平均值，返回一个CPU上的标量值\n",
    "        return all_t.mean().cpu().item()\n",
    "            \n",
    "def train(args, model, train_dataloader, optimizer, scheduler, local_rank, rank, world_size, tokenizer):\n",
    "    rm = RunningMean(local_rank)\n",
    "    global_step = 0\n",
    "\n",
    "    if global_step == 0 and is_master(rank):\n",
    "        # Initialize TensorBoard writer only on the master process\n",
    "        print('init tensorboard,', args.experiment_name)\n",
    "        writer = SummaryWriter(args.log_dir)\n",
    "        # with open(os.path.join(args.output_dir, \"log.csv\"), 'w') as f_log:\n",
    "        #     f_log.write('global_step, total_loss\\n')\n",
    "\n",
    "    gradient_accumulation_steps = args.gradient_accumulation_steps  # bs = 10\n",
    "    for epoch in range(args.num_epochs):\n",
    "        for step, data in tqdm(enumerate(train_dataloader), total=len(train_dataloader), disable=not is_master(rank), desc=f'Epoch {epoch}/{args.num_epochs}'):\n",
    "            model.train()\n",
    "                    \n",
    "            loss = model(**data).loss\n",
    "            # accululating gradients over steps\n",
    "            if gradient_accumulation_steps > 1:\n",
    "                loss = loss / gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "            if step % gradient_accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "            \n",
    "            rm.add(loss.item())\n",
    "            mean_loss = rm.mean_all_rank()\n",
    "            if is_master(rank):\n",
    "                print(f'loss: {mean_loss}')\n",
    "                writer.add_scalar('Loss/train', mean_loss, global_step)      \n",
    "                # with open(os.path.join(args.output_dir, \"log.csv\"), \"a\") as f_log:\n",
    "                #     f_log.write(str(global_step)+\",\"+str(mean_loss)+\"\\n\")\n",
    "            \n",
    "            if(global_step%(args.save_checkpoint_steps*gradient_accumulation_steps)==0 and global_step>0):\n",
    "                dir_name = 'model_'+str(global_step)\n",
    "                checkpoint_dir = os.path.join(args.output_dir, dir_name)\n",
    "                if not os.path.exists(checkpoint_dir):\n",
    "                    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "                print(\"saving checkpoint...\")\n",
    "                utils.save_model_checkpoint(model, checkpoint_dir, rank)\n",
    "                \n",
    "                \n",
    "            dist.barrier()\n",
    "            global_step += 1\n",
    "        \n",
    "    if is_master(rank):\n",
    "        writer.flush()\n",
    "        writer.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # parser.add_argument('--load_from', type=str, default='/data/local/IndexQuality/FinetuneLLM/Phi-3-medium')\n",
    "    parser.add_argument('--load_from', type=str, default='intfloat/e5-mistral-7b-instruct')\n",
    "    parser.add_argument('--model_path', type=str, default='/cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/Mixtral_New_ym_e5/current_best_1200/pytorch_model.bin')\n",
    "    parser.add_argument('--model', type=str, default='mixtral')\n",
    "    parser.add_argument('--num_workers', type=int, default=4)\n",
    "    parser.add_argument('--warmup', type=float, default=0.1)\n",
    "    parser.add_argument('--training_data_from', type=str, default=\"/cosmos/local/IndexQuality/FinetuneLLM/TrainingData/O1_A3_crowd_llm_training_data_no_eval_dataset_overlap.tsv\")\n",
    "    parser.add_argument('--columns', '-c', nargs='+', required=True, help=\"Names of columns to read\")\n",
    "    parser.add_argument('--batch_size', type=int, default=10) # 8->10 \n",
    "    parser.add_argument('--gradient_accumulation_steps', type=int, default=2)\n",
    "    parser.add_argument('--num_epochs', type=int, default=2)\n",
    "    parser.add_argument('--max_seq_length', type=int, default=1024)\n",
    "    # parser.add_argument('--eval_steps', type=int, default=200)  # 平常隔200评估一次，如果效果比之前好，直接保存；否则按save_checkpoint_steps保存\n",
    "    parser.add_argument('--save_checkpoint_steps', type=int, default=100)\n",
    "    parser.add_argument('--lr', type=float, default=1e-5)\n",
    "    parser.add_argument('--gpu_counts', type=float, default=16) # warmup=0.1\n",
    "    parser.add_argument('--output_dir', type=str, required=True)\n",
    "    parser.add_argument('--experiment_name', type=str, required=True)\n",
    "    parser.add_argument('--log_dir', type=str, required=True)\n",
    "    args = parser.parse_args()\n",
    "    train(args, *setup(args))\n",
    "    \n",
    "# singularity Command\n",
    "# pip install transformers[torch]==4.38.1 datasets scikit-learn dataclasses lightgbm matplotlib mlflow tensorboard && cd /cosmos/local/IndexQuality/ContentModels/DataAugmentation/data/CBSpam_v3/Code/FinetuneLLM-US/new/ && CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 torchrun --nnodes 1 --nproc_per_node 16 train_script_optimized_test.py --load_from /cosmos/local/IndexQuality/FinetuneLLM/Mixtral-8x7B-Instruct-v0.1/ --model mixtral --num_workers 4 --training_data_from /cosmos/local/IndexQuality/FinetuneLLM/TrainingData/O1_A3_crowd_llm_training_data_no_eval_dataset_overlap.tsv --batch_size 10 --gradient_accumulation_steps 2 --num_epochs 2 --save_checkpoint_steps 200 --lr 1e-5 --output_dir /cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/Mixtral_New_ym_bs_10_test --experiment_name MixtralTrain_ym --disable_tensorboard False --log_dir /cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/Mixtral_New_ym_bs_10_test/log\n",
    "# current running: Mixtral_again: https://ml.azure.com/runs/Mixtral_again?wsid=/subscriptions/7972af26-e54d-410e-a755-20e582a46de0/resourceGroups/singularity-webdata/providers/Microsoft.MachineLearningServices/workspaces/singularity-webdata-ws01-eastus2&flight=1ptraining&tid=72f988bf-86f1-41af-91ab-2d7cd011db47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:__main__:\n",
      "*****************************************\n",
      "Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "*****************************************\n",
      "[2024-08-28 03:23:20,172] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-08-28 03:23:20,173] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-08-28 03:23:20,192] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:118: UserWarning: onnxruntime training package info: package_name: onnxruntime-training\n",
      "  warnings.warn(\"onnxruntime training package info: package_name: %s\" % package_name)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:119: UserWarning: onnxruntime training package info: __version__: 1.18.0\n",
      "  warnings.warn(\"onnxruntime training package info: __version__: %s\" % version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:120: UserWarning: onnxruntime training package info: cuda_version: 12.2\n",
      "  warnings.warn(\"onnxruntime training package info: cuda_version: %s\" % cuda_version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:121: UserWarning: onnxruntime build info: cudart_version: 12020\n",
      "  warnings.warn(\"onnxruntime build info: cudart_version: %s\" % cudart_version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:129: UserWarning: WARNING: failed to find cudart version that matches onnxruntime build info\n",
      "  warnings.warn(\"WARNING: failed to find cudart version that matches onnxruntime build info\")\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:130: UserWarning: WARNING: found cudart versions: [12040]\n",
      "  warnings.warn(\"WARNING: found cudart versions: %s\" % local_cudart_versions)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:118: UserWarning: onnxruntime training package info: package_name: onnxruntime-training\n",
      "  warnings.warn(\"onnxruntime training package info: package_name: %s\" % package_name)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:119: UserWarning: onnxruntime training package info: __version__: 1.18.0\n",
      "  warnings.warn(\"onnxruntime training package info: __version__: %s\" % version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:120: UserWarning: onnxruntime training package info: cuda_version: 12.2\n",
      "  warnings.warn(\"onnxruntime training package info: cuda_version: %s\" % cuda_version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:121: UserWarning: onnxruntime build info: cudart_version: 12020\n",
      "  warnings.warn(\"onnxruntime build info: cudart_version: %s\" % cudart_version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:129: UserWarning: WARNING: failed to find cudart version that matches onnxruntime build info\n",
      "  warnings.warn(\"WARNING: failed to find cudart version that matches onnxruntime build info\")\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:130: UserWarning: WARNING: found cudart versions: [12040]\n",
      "  warnings.warn(\"WARNING: found cudart versions: %s\" % local_cudart_versions)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:118: UserWarning: onnxruntime training package info: package_name: onnxruntime-training\n",
      "  warnings.warn(\"onnxruntime training package info: package_name: %s\" % package_name)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:119: UserWarning: onnxruntime training package info: __version__: 1.18.0\n",
      "  warnings.warn(\"onnxruntime training package info: __version__: %s\" % version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:120: UserWarning: onnxruntime training package info: cuda_version: 12.2\n",
      "  warnings.warn(\"onnxruntime training package info: cuda_version: %s\" % cuda_version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:121: UserWarning: onnxruntime build info: cudart_version: 12020\n",
      "  warnings.warn(\"onnxruntime build info: cudart_version: %s\" % cudart_version)\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:129: UserWarning: WARNING: failed to find cudart version that matches onnxruntime build info\n",
      "  warnings.warn(\"WARNING: failed to find cudart version that matches onnxruntime build info\")\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/onnxruntime/capi/onnxruntime_validation.py:130: UserWarning: WARNING: found cudart versions: [12040]\n",
      "  warnings.warn(\"WARNING: found cudart versions: %s\" % local_cudart_versions)\n",
      "World size: 3, rank: 2, local rank: 2\n",
      "[W828 03:23:25.766221319 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "World size: 3, rank: 1, local rank: 1\n",
      "[W828 03:23:25.218545857 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())\n",
      "tokenizer_config.json: 100%|███████████████████| 981/981 [00:00<00:00, 7.99MB/s]\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "World size: 3, rank: 0, local rank: 0\n",
      "[W828 03:23:26.600698827 Utils.hpp:135] Warning: Environment variable NCCL_ASYNC_ERROR_HANDLING is deprecated; use TORCH_NCCL_ASYNC_ERROR_HANDLING instead (function operator())\n",
      "Namespace(load_from='intfloat/e5-mistral-7b-instruct', model_path='/cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/Mixtral_New_ym_e5/current_best_1200/pytorch_model.bin', model='mistral', num_workers=4, warmup=0.1, training_data_from='/cosmos/local/IndexQuality/FinetuneLLM/TrainingData/crowd_label_o1_for_training.csv', columns=['Url', 'UrlTitle', 'UrlSnippet', 'FullBody', 'Label'], batch_size=20, gradient_accumulation_steps=1, num_epochs=2, max_seq_length=1024, save_checkpoint_steps=100, lr=1e-05, gpu_counts=16.0, output_dir='/cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/e5_crowd_label_v1', experiment_name='e5_crowd_v1', log_dir='/cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/e5_crowd_label_v1/logs')\n",
      "tokenizer.model: 100%|███████████████████████| 493k/493k [00:00<00:00, 68.0MB/s]\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "tokenizer.json: 100%|██████████████████████| 1.80M/1.80M [00:00<00:00, 47.9MB/s]\n",
      "added_tokens.json: 100%|██████████████████████| 42.0/42.0 [00:00<00:00, 448kB/s]\n",
      "special_tokens_map.json: 100%|█████████████████| 168/168 [00:00<00:00, 2.14MB/s]\n",
      "config.json: 100%|█████████████████████████████| 629/629 [00:00<00:00, 3.66MB/s]\n",
      "model.safetensors.index.json: 100%|████████| 23.3k/23.3k [00:00<00:00, 89.8MB/s]\n",
      "Downloading shards:   0%|                                 | 0/2 [00:00<?, ?it/s]\n",
      "model-00001-of-00002.safetensors:   0%|             | 0.00/9.94G [00:00<?, ?B/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|     | 52.4M/9.94G [00:00<00:21, 470MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   1%|      | 105M/9.94G [00:00<00:20, 482MB/s]\u001b[A\n",
      "Downloading shards:   0%|                                 | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Downloading shards:   0%|                                 | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|▏     | 262M/9.94G [00:00<00:21, 448MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   3%|▏     | 325M/9.94G [00:00<00:19, 483MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   4%|▏     | 388M/9.94G [00:00<00:18, 513MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|▎     | 451M/9.94G [00:00<00:17, 535MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   5%|▎     | 514M/9.94G [00:01<00:17, 544MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   6%|▎     | 577M/9.94G [00:01<00:16, 552MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   6%|▍     | 640M/9.94G [00:01<00:17, 547MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   7%|▍     | 703M/9.94G [00:01<00:16, 563MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|▍     | 765M/9.94G [00:01<00:16, 557MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   8%|▍     | 828M/9.94G [00:01<00:15, 570MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:   9%|▌     | 891M/9.94G [00:01<00:15, 572MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10%|▌     | 954M/9.94G [00:01<00:15, 578MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  10%|▌    | 1.03G/9.94G [00:01<00:14, 598MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  11%|▌    | 1.10G/9.94G [00:02<00:14, 600MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|▌    | 1.16G/9.94G [00:02<00:15, 581MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  12%|▌    | 1.23G/9.94G [00:02<00:15, 581MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  13%|▋    | 1.29G/9.94G [00:02<00:14, 585MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  14%|▋    | 1.35G/9.94G [00:02<00:14, 590MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  14%|▋    | 1.42G/9.94G [00:02<00:14, 587MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  15%|▋    | 1.48G/9.94G [00:02<00:14, 594MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|▊    | 1.55G/9.94G [00:02<00:13, 629MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  16%|▊    | 1.63G/9.94G [00:02<00:13, 632MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  17%|▊    | 1.70G/9.94G [00:03<00:13, 629MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  18%|▉    | 1.77G/9.94G [00:03<00:12, 631MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19%|▉    | 1.85G/9.94G [00:03<00:12, 627MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  19%|▉    | 1.91G/9.94G [00:03<00:13, 602MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|▉    | 1.97G/9.94G [00:03<00:13, 595MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  20%|█    | 2.03G/9.94G [00:03<00:13, 604MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  21%|█    | 2.11G/9.94G [00:03<00:12, 625MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|█    | 2.17G/9.94G [00:03<00:13, 565MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  22%|█    | 2.23G/9.94G [00:03<00:13, 580MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  23%|█▏   | 2.31G/9.94G [00:04<00:12, 621MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  24%|█▏   | 2.38G/9.94G [00:04<00:11, 632MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  25%|█▏   | 2.45G/9.94G [00:04<00:11, 637MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  25%|█▎   | 2.53G/9.94G [00:04<00:11, 640MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  26%|█▎   | 2.60G/9.94G [00:04<00:11, 653MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  27%|█▎   | 2.67G/9.94G [00:04<00:11, 639MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28%|█▍   | 2.75G/9.94G [00:04<00:10, 656MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  28%|█▍   | 2.82G/9.94G [00:04<00:10, 662MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  29%|█▍   | 2.89G/9.94G [00:04<00:10, 643MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  30%|█▍   | 2.97G/9.94G [00:05<00:11, 583MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  30%|█▌   | 3.03G/9.94G [00:05<00:12, 574MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  31%|█▌   | 3.09G/9.94G [00:05<00:12, 567MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  32%|█▌   | 3.17G/9.94G [00:05<00:11, 590MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  32%|█▌   | 3.23G/9.94G [00:05<00:11, 596MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  33%|█▋   | 3.29G/9.94G [00:05<00:11, 597MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|█▋   | 3.36G/9.94G [00:05<00:11, 583MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  34%|█▋   | 3.42G/9.94G [00:05<00:11, 560MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  35%|█▊   | 3.48G/9.94G [00:05<00:11, 547MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  36%|█▊   | 3.54G/9.94G [00:06<00:11, 561MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  36%|█▊   | 3.61G/9.94G [00:06<00:10, 579MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  37%|█▊   | 3.67G/9.94G [00:06<00:10, 577MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  38%|█▉   | 3.73G/9.94G [00:06<00:11, 545MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  38%|█▉   | 3.80G/9.94G [00:06<00:13, 470MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  39%|█▉   | 3.85G/9.94G [00:06<00:13, 448MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  39%|█▉   | 3.91G/9.94G [00:06<00:12, 482MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  40%|█▉   | 3.97G/9.94G [00:06<00:11, 512MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  41%|██   | 4.04G/9.94G [00:07<00:11, 509MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  41%|██   | 4.10G/9.94G [00:07<00:11, 529MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42%|██   | 4.16G/9.94G [00:07<00:10, 548MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  42%|██   | 4.23G/9.94G [00:07<00:10, 550MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  43%|██▏  | 4.29G/9.94G [00:07<00:10, 526MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  44%|██▏  | 4.35G/9.94G [00:07<00:10, 521MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  44%|██▏  | 4.41G/9.94G [00:07<00:10, 538MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  45%|██▎  | 4.48G/9.94G [00:07<00:10, 511MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  46%|██▎  | 4.53G/9.94G [00:08<00:10, 509MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  46%|██▎  | 4.59G/9.94G [00:08<00:10, 512MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  47%|██▎  | 4.66G/9.94G [00:08<00:09, 529MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  47%|██▎  | 4.72G/9.94G [00:08<00:09, 534MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  48%|██▍  | 4.78G/9.94G [00:08<00:09, 544MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49%|██▍  | 4.85G/9.94G [00:08<00:08, 566MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  49%|██▍  | 4.92G/9.94G [00:08<00:09, 532MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  50%|██▌  | 4.98G/9.94G [00:08<00:09, 525MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  51%|██▌  | 5.04G/9.94G [00:08<00:09, 519MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  51%|██▌  | 5.11G/9.94G [00:09<00:09, 537MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  52%|██▌  | 5.17G/9.94G [00:09<00:08, 539MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53%|██▋  | 5.23G/9.94G [00:09<00:08, 550MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  53%|██▋  | 5.30G/9.94G [00:09<00:08, 541MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  54%|██▋  | 5.36G/9.94G [00:09<00:08, 522MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55%|██▋  | 5.42G/9.94G [00:09<00:08, 540MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  55%|██▊  | 5.48G/9.94G [00:09<00:07, 559MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  56%|██▊  | 5.56G/9.94G [00:09<00:07, 600MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  57%|██▊  | 5.63G/9.94G [00:09<00:07, 590MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  57%|██▊  | 5.69G/9.94G [00:10<00:08, 519MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  58%|██▉  | 5.76G/9.94G [00:10<00:08, 520MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  59%|██▉  | 5.82G/9.94G [00:10<00:08, 515MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  59%|██▉  | 5.88G/9.94G [00:10<00:07, 538MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  60%|██▉  | 5.95G/9.94G [00:10<00:07, 559MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  60%|███  | 6.01G/9.94G [00:10<00:07, 540MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  61%|███  | 6.07G/9.94G [00:10<00:07, 533MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  62%|███  | 6.13G/9.94G [00:10<00:07, 528MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  62%|███  | 6.20G/9.94G [00:11<00:06, 548MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  63%|███▏ | 6.27G/9.94G [00:11<00:06, 593MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64%|███▏ | 6.34G/9.94G [00:11<00:05, 620MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  64%|███▏ | 6.41G/9.94G [00:11<00:05, 614MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  65%|███▎ | 6.48G/9.94G [00:11<00:05, 622MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  66%|███▎ | 6.55G/9.94G [00:11<00:05, 652MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%|███▎ | 6.63G/9.94G [00:11<00:05, 656MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  67%|███▎ | 6.70G/9.94G [00:11<00:05, 644MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  68%|███▍ | 6.77G/9.94G [00:12<00:05, 535MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69%|███▍ | 6.84G/9.94G [00:12<00:06, 473MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  69%|███▍ | 6.90G/9.94G [00:12<00:06, 500MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  70%|███▌ | 6.97G/9.94G [00:12<00:05, 535MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71%|███▌ | 7.04G/9.94G [00:12<00:05, 551MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  71%|███▌ | 7.10G/9.94G [00:12<00:05, 561MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  72%|███▌ | 7.16G/9.94G [00:12<00:04, 559MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73%|███▋ | 7.22G/9.94G [00:12<00:04, 554MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  73%|███▋ | 7.29G/9.94G [00:12<00:04, 563MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  74%|███▋ | 7.35G/9.94G [00:13<00:04, 535MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75%|███▋ | 7.41G/9.94G [00:13<00:07, 345MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  75%|███▊ | 7.49G/9.94G [00:13<00:06, 405MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  76%|███▊ | 7.55G/9.94G [00:13<00:05, 430MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77%|███▊ | 7.61G/9.94G [00:13<00:05, 457MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  77%|███▊ | 7.68G/9.94G [00:13<00:04, 485MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78%|███▉ | 7.74G/9.94G [00:14<00:04, 516MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  78%|███▉ | 7.80G/9.94G [00:14<00:03, 541MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  79%|███▉ | 7.87G/9.94G [00:14<00:03, 571MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80%|███▉ | 7.94G/9.94G [00:14<00:03, 558MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  80%|████ | 8.00G/9.94G [00:14<00:03, 534MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  81%|████ | 8.06G/9.94G [00:14<00:03, 523MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82%|████ | 8.13G/9.94G [00:14<00:03, 491MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  82%|████ | 8.18G/9.94G [00:14<00:03, 474MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83%|████▏| 8.23G/9.94G [00:14<00:03, 464MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  83%|████▏| 8.28G/9.94G [00:15<00:03, 448MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84%|████▏| 8.34G/9.94G [00:15<00:03, 463MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  84%|████▏| 8.39G/9.94G [00:15<00:03, 466MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  85%|████▏| 8.44G/9.94G [00:15<00:03, 471MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  85%|████▎| 8.49G/9.94G [00:15<00:03, 458MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  86%|████▎| 8.55G/9.94G [00:15<00:03, 455MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  87%|████▎| 8.62G/9.94G [00:15<00:02, 508MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  87%|████▎| 8.68G/9.94G [00:15<00:02, 536MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  88%|████▍| 8.76G/9.94G [00:16<00:02, 577MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  89%|████▍| 8.83G/9.94G [00:16<00:01, 591MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  89%|████▍| 8.89G/9.94G [00:16<00:01, 579MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  90%|████▌| 8.97G/9.94G [00:16<00:01, 602MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91%|████▌| 9.03G/9.94G [00:16<00:01, 600MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  91%|████▌| 9.09G/9.94G [00:16<00:01, 541MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  92%|████▌| 9.15G/9.94G [00:16<00:01, 556MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  93%|████▋| 9.23G/9.94G [00:16<00:01, 588MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94%|████▋| 9.30G/9.94G [00:16<00:01, 615MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  94%|████▋| 9.36G/9.94G [00:17<00:00, 595MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95%|████▋| 9.43G/9.94G [00:17<00:00, 595MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  95%|████▊| 9.49G/9.94G [00:17<00:00, 576MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  96%|████▊| 9.55G/9.94G [00:17<00:00, 571MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97%|████▊| 9.62G/9.94G [00:17<00:00, 558MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  97%|████▊| 9.68G/9.94G [00:17<00:00, 564MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  98%|████▉| 9.74G/9.94G [00:17<00:00, 570MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99%|████▉| 9.80G/9.94G [00:17<00:00, 581MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors:  99%|████▉| 9.88G/9.94G [00:17<00:00, 620MB/s]\u001b[A\n",
      "model-00001-of-00002.safetensors: 100%|█████| 9.94G/9.94G [00:18<00:00, 551MB/s]\u001b[A\n",
      "Downloading shards:  50%|████████████▌            | 1/2 [00:17<00:17, 17.62s/it]\n",
      "model-00002-of-00002.safetensors:   0%|             | 0.00/4.28G [00:00<?, ?B/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   1%|     | 62.9M/4.28G [00:00<00:06, 617MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   3%|▏     | 136M/4.28G [00:00<00:06, 681MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   5%|▎     | 210M/4.28G [00:00<00:05, 702MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   7%|▍     | 283M/4.28G [00:00<00:06, 635MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:   8%|▍     | 357M/4.28G [00:00<00:06, 611MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  10%|▌     | 419M/4.28G [00:00<00:06, 605MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  11%|▋     | 482M/4.28G [00:00<00:06, 603MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  13%|▊     | 545M/4.28G [00:00<00:06, 606MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  14%|▊     | 619M/4.28G [00:00<00:05, 628MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  16%|▉     | 692M/4.28G [00:01<00:05, 647MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  18%|█     | 765M/4.28G [00:01<00:05, 665MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  20%|█▏    | 839M/4.28G [00:01<00:05, 685MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  21%|█▎    | 912M/4.28G [00:01<00:04, 697MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  23%|█▍    | 986M/4.28G [00:01<00:05, 630MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  25%|█▏   | 1.06G/4.28G [00:01<00:04, 644MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  26%|█▎   | 1.13G/4.28G [00:01<00:04, 664MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  28%|█▍   | 1.21G/4.28G [00:01<00:04, 661MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  30%|█▍   | 1.28G/4.28G [00:02<00:04, 611MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  31%|█▌   | 1.34G/4.28G [00:02<00:04, 612MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  33%|█▋   | 1.41G/4.28G [00:02<00:04, 615MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  34%|█▋   | 1.47G/4.28G [00:02<00:04, 596MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  36%|█▊   | 1.54G/4.28G [00:02<00:04, 621MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  38%|█▉   | 1.61G/4.28G [00:02<00:04, 641MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  39%|█▉   | 1.69G/4.28G [00:02<00:04, 635MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  41%|██   | 1.76G/4.28G [00:02<00:03, 636MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  43%|██▏  | 1.84G/4.28G [00:02<00:03, 636MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  45%|██▏  | 1.91G/4.28G [00:03<00:03, 604MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  46%|██▎  | 1.97G/4.28G [00:03<00:03, 596MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  48%|██▍  | 2.04G/4.28G [00:03<00:03, 626MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  50%|██▍  | 2.12G/4.28G [00:03<00:03, 650MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  51%|██▌  | 2.19G/4.28G [00:03<00:03, 662MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  53%|██▋  | 2.26G/4.28G [00:03<00:03, 665MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  55%|██▋  | 2.34G/4.28G [00:03<00:02, 682MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  56%|██▊  | 2.41G/4.28G [00:03<00:02, 695MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  58%|██▉  | 2.49G/4.28G [00:03<00:02, 654MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  60%|██▉  | 2.56G/4.28G [00:04<00:02, 592MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  62%|███  | 2.63G/4.28G [00:04<00:02, 624MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  63%|███▏ | 2.71G/4.28G [00:04<00:02, 612MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  65%|███▏ | 2.77G/4.28G [00:04<00:02, 595MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  66%|███▎ | 2.83G/4.28G [00:04<00:02, 587MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  68%|███▍ | 2.89G/4.28G [00:04<00:02, 588MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  69%|███▍ | 2.97G/4.28G [00:04<00:02, 610MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  71%|███▌ | 3.03G/4.28G [00:04<00:02, 596MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  72%|███▌ | 3.09G/4.28G [00:04<00:02, 588MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  74%|███▋ | 3.16G/4.28G [00:05<00:01, 590MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  75%|███▊ | 3.22G/4.28G [00:05<00:01, 598MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  77%|███▊ | 3.29G/4.28G [00:05<00:01, 621MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  79%|███▉ | 3.37G/4.28G [00:05<00:01, 646MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  80%|████ | 3.44G/4.28G [00:05<00:01, 662MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  82%|████ | 3.51G/4.28G [00:05<00:01, 654MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  84%|████▏| 3.59G/4.28G [00:05<00:01, 666MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  86%|████▎| 3.66G/4.28G [00:05<00:00, 682MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  87%|████▎| 3.73G/4.28G [00:05<00:00, 693MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  89%|████▍| 3.81G/4.28G [00:05<00:00, 704MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  91%|████▌| 3.88G/4.28G [00:06<00:00, 712MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  92%|████▌| 3.95G/4.28G [00:06<00:00, 717MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  94%|████▋| 4.04G/4.28G [00:06<00:00, 726MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  96%|████▊| 4.11G/4.28G [00:06<00:00, 723MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors:  98%|████▉| 4.18G/4.28G [00:06<00:00, 719MB/s]\u001b[A\n",
      "model-00002-of-00002.safetensors: 100%|█████| 4.28G/4.28G [00:06<00:00, 645MB/s]\u001b[A\n",
      "Downloading shards: 100%|█████████████████████████| 2/2 [00:25<00:00, 12.70s/it]\n",
      "Downloading shards: 100%|█████████████████████████| 2/2 [00:24<00:00, 12.26s/it]\n",
      "Downloading shards: 100%|█████████████████████████| 2/2 [00:24<00:00, 12.23s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:02<00:00,  1.13s/it]\n",
      "Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at intfloat/e5-mistral-7b-instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "--> applying fsdp activation checkpointing...\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:03<00:00,  1.84s/it]\n",
      "Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at intfloat/e5-mistral-7b-instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:03<00:00,  1.78s/it]\n",
      "Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at intfloat/e5-mistral-7b-instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "model config: MistralConfig {\n",
      "  \"_name_or_path\": \"intfloat/e5-mistral-7b-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"MistralModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"model_type\": \"mistral\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 4096,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.38.1\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "MistralForSequenceClassification(\n",
      "  (model): MistralModel(\n",
      "    (embed_tokens): Embedding(32000, 4096, padding_idx=2)\n",
      "    (layers): ModuleList(\n",
      "      (0-31): 32 x MistralDecoderLayer(\n",
      "        (self_attn): MistralSdpaAttention(\n",
      "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "          (rotary_emb): MistralRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): MistralMLP(\n",
      "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
      "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): MistralRMSNorm()\n",
      "        (post_attention_layernorm): MistralRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): MistralRMSNorm()\n",
      "  )\n",
      "  (score): Linear(in_features=4096, out_features=2, bias=False)\n",
      ")\n",
      "--> applying fsdp activation checkpointing...\n",
      "--> applying fsdp activation checkpointing...\n",
      "training dataset length: 176491\n",
      "training data columns: Index(['Url', 'UrlTitle', 'UrlSnippet', 'FullBody', 'Label'], dtype='object')\n",
      "training data pos/neg counts: Label\n",
      "0    173513\n",
      "1      2978\n",
      "Name: count, dtype: int64\n",
      "training dataset length: 176491\n",
      "training data columns: Index(['Url', 'UrlTitle', 'UrlSnippet', 'FullBody', 'Label'], dtype='object')\n",
      "training data pos/neg counts: Label\n",
      "0    173513\n",
      "1      2978\n",
      "Name: count, dtype: int64\n",
      "init tensorboard, e5_crowd_v1\n",
      "training dataset length: 176491\n",
      "training data columns: Index(['Url', 'UrlTitle', 'UrlSnippet', 'FullBody', 'Label'], dtype='object')\n",
      "training data pos/neg counts: Label\n",
      "0    173513\n",
      "1      2978\n",
      "Name: count, dtype: int64\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 0/2:   0%|                                       | 0/2941 [00:00<?, ?it/s]You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "NCCL version 2.19.4+cuda12.4\n",
      "\n",
      "node-0:13737:15972 [0] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:13737:15972 [0] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_0\n",
      "\n",
      "node-0:13737:15972 [0] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:13737:15972 [0] transport/net_ib.cc:193 NCCL WARN NET/IB : Unable to open device mlx5_0\n",
      "\n",
      "node-0:13738:15975 [1] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:13738:15975 [1] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_0\n",
      "\n",
      "node-0:13738:15975 [1] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:13738:15975 [1] transport/net_ib.cc:193 NCCL WARN NET/IB : Unable to open device mlx5_0\n",
      "\n",
      "node-0:13739:15977 [2] ibvwrap.c:66 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:13739:15977 [2] p2p_plugin.c:233 NCCL WARN NET/IB : Unable to open device mlx5_0\n",
      "\n",
      "node-0:13739:15977 [2] misc/ibvwrap.cc:113 NCCL WARN Call to ibv_open_device failed\n",
      "\n",
      "node-0:13739:15977 [2] transport/net_ib.cc:193 NCCL WARN NET/IB : Unable to open device mlx5_0\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "/opt/conda/envs/ptca/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n",
      "loss: 3.9791667461395264\n",
      "Epoch 0/2:   0%|                            | 1/2941 [00:15<12:20:50, 15.12s/it]loss: 4.4296875\n",
      "Epoch 0/2:   0%|                            | 2/2941 [00:28<11:34:24, 14.18s/it]^C\n",
      "W0828 03:24:54.111000 140035355010240 torch/distributed/elastic/agent/server/api.py:688] Received Signals.SIGINT death signal, shutting down workers\n",
      "W0828 03:24:54.112000 140035355010240 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 13737 closing signal SIGINT\n",
      "W0828 03:24:54.112000 140035355010240 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 13738 closing signal SIGINT\n",
      "W0828 03:24:54.112000 140035355010240 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 13739 closing signal SIGINT\n"
     ]
    }
   ],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=\"1,2,3\"\n",
    "!NCCL_DEBUG=WARN python -m torch.distributed.run  \\\n",
    "--master_port 29501 --nnodes 1 --nproc_per_node 3 train_script_optimized_e5_8.py \\\n",
    "--load_from intfloat/e5-mistral-7b-instruct \\\n",
    "--model mistral --num_workers 4 \\\n",
    "--training_data_from /cosmos/local/IndexQuality/FinetuneLLM/TrainingData/crowd_label_o1_for_training.csv \\\n",
    "--columns Url UrlTitle UrlSnippet FullBody Label \\\n",
    "--batch_size 20 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--num_epochs 2 \\\n",
    "--max_seq_length 1024 \\\n",
    "--save_checkpoint_steps 100 \\\n",
    "--lr 1e-5 \\\n",
    "--gpu_counts 16 \\\n",
    "--output_dir /cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/e5_crowd_label_v1 \\\n",
    "--experiment_name e5_crowd_v1 \\\n",
    "--log_dir /cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/e5_crowd_label_v1/logs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train_script_prompt_e5_0.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train_script_prompt_e5_0.py\n",
    "# %%writefile train_script_optimized_e5_2.py\n",
    "# %%writefile train_script_optimized_e5_3.py\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "import json\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "import shutil\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DistributedSampler,Dataset, DataLoader\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import MixedPrecision\n",
    "from transformers.models.mixtral.modeling_mixtral import MixtralDecoderLayer\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification, AutoModelForCausalLM, default_data_collator, get_cosine_schedule_with_warmup, AutoConfig, BitsAndBytesConfig\n",
    "import gc\n",
    "from transformers import MistralForCausalLM\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import subprocess\n",
    "import utils\n",
    "import copy\n",
    "import pandas as pd\n",
    "\n",
    "from torch.nn import functional as F\n",
    "\n",
    "prompt_part1 = \\\n",
    "f'''You are a website spam expert. You are given information about a webpage to judge whether or not it is spam. 0 means nonspam and 1 means spam. Give your prediction after the <ANS>: tag.\n",
    "    Url: {{Url}}\n",
    "    UrlTitle: {{UrlTitle}}\n",
    "    UrlSnippet: {{UrlSnippet}} \n",
    "    Site Content: {{FullBody}}\n",
    "'''\n",
    "\n",
    "prompt_part2 = \\\n",
    "f'''\n",
    "What is your prediction <ANS>:{{Label}}'''\n",
    "\n",
    "IGNORE_INDEX = -100  # The default setting in CrossEntropyLoss\n",
    "# MAX_LENGTH_EVAL = 1024\n",
    "\n",
    "class TrainingDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_seq_length):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        def apply_prompt1_template(sample):\n",
    "            return prompt_part1.format(Url=sample['Url'],\n",
    "                                        UrlTitle=sample['UrlTitle'],\n",
    "                                        UrlSnippet=sample['UrlSnippet'],\n",
    "                                        FullBody=sample['FullBody'])\n",
    "        \n",
    "        def apply_prompt2_template(sample):\n",
    "            return prompt_part2.format(Label=sample['Label'])\n",
    "        \n",
    "        \n",
    "        row = self.df.iloc[idx]\n",
    "        text_part_1 = apply_prompt1_template(row[[\"Url\",\"UrlTitle\",\"UrlSnippet\",\"FullBody\"]])\n",
    "        text_part_2 = apply_prompt2_template(row[[\"Label\"]])\n",
    "\n",
    "        res = self.tokenizer(f\"{self.tokenizer.bos_token} {text_part_1}\", text_part_2, add_special_tokens=False, max_length=self.max_seq_length, padding='max_length', truncation='only_first')\n",
    "        \n",
    "        labels = torch.tensor(copy.deepcopy(res['input_ids']), dtype=torch.int64)\n",
    "        actual_token_len = sum(res['attention_mask'])\n",
    "\n",
    "        labels[:actual_token_len-1] = IGNORE_INDEX  \n",
    "        labels[actual_token_len:] = IGNORE_INDEX\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(res['input_ids']),\n",
    "            'attention_mask': torch.tensor(res['attention_mask']),\n",
    "            'labels':labels\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "def setup(args):\n",
    "    # setup distributed environment\n",
    "    world_size = int(os.environ[\"WORLD_SIZE\"])  # 参与训练的总进程数，即GPU数量。在分布式训练中，每个GPU对应一个进程。因此，world_size是你在分布式训练中使用的GPU数量\n",
    "    rank = int(os.environ[\"RANK\"])      # rank是每个进程的唯一标识符，用于区分不同的进程。在分布式训练中，每个进程都有一个独特的rank，从0到world_size-1\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])  # local_rank:是在单个节点内部的进程的本地标识符。如果你在多个节点上运行分布式训练，每个节点都有自己的local_rank。\n",
    "    # 比如，你在2台服务器上运行分布式训练，每台服务器有4个GPU。那么总的world_size是8，进程的rank分别是[0,1,2,3,4,5,6,7]。在每台服务器上local_rank分别是[0,1,2,3]\n",
    "    print(f\"World size: {world_size}, rank: {rank}, local rank: {local_rank}\")\n",
    "\n",
    "    timeout = timedelta(hours=5) # 时间间隔对象，表示在初始化分布式进程组时等待的最大时间。这里表示如果在5小时内无法成功初始化分布式进程组，将引发超时错误\n",
    "    dist.init_process_group(\"nccl\", timeout=timeout, rank=rank, world_size=world_size) # 初始化分布式进程组，nccl表示用用NVIDIA Collective Communications Library作为后端，rank表示当前进程的标识符，world_size表示总进程数\n",
    "    assert torch.distributed.is_initialized()\n",
    "\n",
    "    torch.cuda.set_device(local_rank)  # 设置当前进程使用的GPU设备，local_rank表示当前节点内部的进程的本地标识符\n",
    "    torch.cuda.empty_cache()    # 用于清空GPU缓存，以释放内存\n",
    "\n",
    "    if is_master(rank): # 只有当前是主进程时才会执行打印参数的操作\n",
    "        print(args)\n",
    "    \n",
    "    # setup tokenizer and dataloader\n",
    "    \n",
    "    # if args.model == 'llama2':\n",
    "    #     tokenizer = LlamaTokenizer.from_pretrained(args.load_from)\n",
    "    # elif args.model == 'mistral':\n",
    "    #     tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "    # elif args.model == 'turing':\n",
    "    #     # tokenizer = TNLGv4Tokenizer()\n",
    "    #     tokenizer = GPT2TokenizerFast.from_pretrained('Xenova/gpt-4')\n",
    "    # elif args.model == 'mixtral':\n",
    "    #     tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mixtral-8x7B-v0.1\")\n",
    "    # else:\n",
    "    #     print(f\"model {args.model} not supported\")\n",
    "    #     raise NotImplementedError\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.load_from,add_bos_token=True)\n",
    "    tokenizer.padding_side = 'left'\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # setup model and fsdp\n",
    "    if args.model == 'llama2':\n",
    "        model = LlamaForCausalLM.from_pretrained(args.load_from, load_in_8bit=False, device_map=None, torch_dtype=torch.bfloat16, use_cache=True)\n",
    "        if args.neftune_alpha is not None:\n",
    "            print('NEFTUNE enabled')\n",
    "            # Save the old forward function as a class attribute\n",
    "            torch.nn.Embedding.old_forward = model.model.embed_tokens.forward\n",
    "\n",
    "            # Define the new forward function\n",
    "            def new_forward(self, x):\n",
    "                # Call the old forward function and get its output\n",
    "                #print('neftune forward')\n",
    "                out = self.old_forward(x)\n",
    "                dims = torch.tensor(out.size(1) * out.size(2))\n",
    "                mag_norm = args.neftune_alpha / torch.sqrt(dims)\n",
    "                return out + torch.zeros_like(out).uniform_(-mag_norm, mag_norm)\n",
    "\n",
    "            # Replace the forward function of the embedding object with the new one\n",
    "            model.model.embed_tokens.forward = new_forward.__get__(model.model.embed_tokens, torch.nn.Embedding)\n",
    "    elif args.model == \"mistral\":\n",
    "        # e5\n",
    "        # model = MistralForCausalLM.from_pretrained(args.load_from, load_in_8bit=False, device_map=None, torch_dtype=torch.float16, use_cache=True)\n",
    "        # model = AutoModel.from_pretrained(args.load_from, load_in_8bit=False, device_map=None, torch_dtype=torch.float16)  # device_map=f'cuda:{local_rank}'\n",
    "        model = AutoModel.from_pretrained(args.load_from, load_in_8bit=False, device_map=f'cuda:{local_rank}', torch_dtype=torch.float16) \n",
    "        # model = AutoModelForSequenceClassification.from_pretrained(args.load_from, num_labels=2, load_in_8bit=False, device_map=f'cuda:{local_rank}', torch_dtype=torch.float16) # zhym\n",
    "        # model_state_dict = torch.load(args.model_path)\n",
    "        # model.load_state_dict(model_state_dict)\n",
    "    elif args.model == \"mixtral\":\n",
    "        model = AutoModelForCausalLM.from_pretrained(args.load_from,\n",
    "                                                    load_in_8bit=False, # 让模型不要以8位精度加载，使用默认精度加载权重\n",
    "                                                    device_map=None,    # 没有自定义的设备映射，此时如果你在多个GPU上运行，模型的参数将均匀分配到所有可用的GPU上，适用于参数量大的模型\n",
    "                                                    torch_dtype=torch.bfloat16,  # 用16位浮点数作为权重数据类型\n",
    "                                                    use_cache=True  # 模型将使用缓存来加速计算\n",
    "                                                    )\n",
    "    else:\n",
    "        print(f\"model {args.model} not supported\")\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    if is_master(rank):\n",
    "        print(f\"model config: {model.config}\")\n",
    "        print(model)\n",
    "        \n",
    "    # model.config.pad_token_id = model.config.bos_token_id  # 先注释掉看看\n",
    "    model.to(dtype=torch.bfloat16)\n",
    "\n",
    "    #utils.apply_fsdp_checkpointing(model) if args.model == 'llama2' else model.gradient_checkpointing_enable() # for turing\n",
    "    # model.gradient_checkpointing_enable()\n",
    "    #print(f\"Gradient Checkpointing: {model.is_gradient_checkpointing}\")\n",
    "\n",
    "    model = FSDP(\n",
    "        model,\n",
    "        auto_wrap_policy=utils.get_mistral_wrapper(),\n",
    "        #mixed_precision=mixed_precision_policy,\n",
    "        mixed_precision=None,\n",
    "        sharding_strategy=utils.fsdp_config.sharding_strategy,\n",
    "        device_id=torch.cuda.current_device(),\n",
    "        limit_all_gathers=True,\n",
    "        sync_module_states=False,\n",
    "        param_init_fn=None\n",
    "    )\n",
    "\n",
    "    # Mistral-E5\n",
    "    utils.apply_fsdp_checkpointing(model) \n",
    "    \n",
    "    # setup optimizer\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=args.lr,\n",
    "        weight_decay=0.0,\n",
    "    )\n",
    "    \n",
    "    # df = pd.read_csv(\"/cosmos/local/IndexQuality/FinetuneLLM/TrainingData/O1_A3_crowd_training_data_1.tsv\",sep=\"\\t\")\n",
    "    df = pd.read_csv(args.training_data_from, sep='\\t', usecols=args.columns)\n",
    "    df = df[df['Label'].notna()]\n",
    "    print(f'training dataset length: {len(df)}')\n",
    "    print(f'training data columns: {df.columns}')\n",
    "    print(f'training data pos/neg counts: {df.Label.value_counts()}')\n",
    "    \n",
    "    train_ds =  TrainingDataset(df, tokenizer, args.max_seq_length)\n",
    "    \n",
    "    train_sampler = DistributedSampler(\n",
    "        train_ds,\n",
    "        rank=rank,\n",
    "        num_replicas=world_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    \n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=args.batch_size,\n",
    "        num_workers=args.num_workers,  # 4\n",
    "        pin_memory=True,\n",
    "        sampler=train_sampler,\n",
    "        drop_last=True,\n",
    "        collate_fn=default_data_collator,\n",
    "    )\n",
    "\n",
    "    total_iteration = args.num_epochs * (len(train_dataloader)/(args.batch_size*args.gpu_counts))\n",
    "    scheduler = get_cosine_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=len(train_dataloader) * args.warmup,\n",
    "        num_training_steps=len(train_dataloader) * args.num_epochs\n",
    "        # num_warmup_steps=total_iteration*0.1,\n",
    "        # num_training_steps=total_iteration\n",
    "    )\n",
    "\n",
    "    return model, train_dataloader, optimizer, scheduler, local_rank, rank, world_size, tokenizer\n",
    "\n",
    "\n",
    "def is_master(rank):\n",
    "    # In a multi-node setup, the master process is rank 0\n",
    "    return rank == 0\n",
    "        \n",
    "class RunningMean(object):\n",
    "    def __init__(self, local_rank, N=100):\n",
    "        self.N = N\n",
    "        self.local_rank = local_rank\n",
    "        self.data = []\n",
    "\n",
    "    def add(self, x):\n",
    "        self.data.append(x)\n",
    "\n",
    "    def mean_all_rank(self):\n",
    "        d = self.data[-self.N:]\n",
    "        if len(d) > 0:\n",
    "            m = sum(d) / len(d)\n",
    "        else:\n",
    "            m = 0\n",
    "        t = torch.tensor([m]).to(self.local_rank) # 将m转换为PyTorch张量，并将其移到self.local_rank对应的设备上\n",
    "        all_t = torch.zeros(dist.get_world_size(), dtype=t.dtype).to(self.local_rank)   # 创建一个全零张量all_t，大小为分布式环境中的进程数\n",
    "        dist.all_gather_into_tensor(all_t, t)   # 将所有进程的m收集到all_t中，并计算所有进程的平均值，返回一个CPU上的标量值\n",
    "        return all_t.mean().cpu().item()\n",
    "            \n",
    "def train(args, model, train_dataloader, optimizer, scheduler, local_rank, rank, world_size, tokenizer):\n",
    "    rm = RunningMean(local_rank)\n",
    "    global_step = 0\n",
    "\n",
    "    if global_step == 0 and is_master(rank):\n",
    "        # Initialize TensorBoard writer only on the master process\n",
    "        print('init tensorboard,', args.experiment_name)\n",
    "        writer = SummaryWriter(args.log_dir)\n",
    "        # with open(os.path.join(args.output_dir, \"log.csv\"), 'w') as f_log:\n",
    "        #     f_log.write('global_step, total_loss\\n')\n",
    "\n",
    "    gradient_accumulation_steps = args.gradient_accumulation_steps  # bs = 10\n",
    "    for epoch in range(args.num_epochs):\n",
    "        for step, data in tqdm(enumerate(train_dataloader), total=len(train_dataloader), disable=not is_master(rank), desc=f'Epoch {epoch}/{args.num_epochs}'):\n",
    "            model.train()\n",
    "                    \n",
    "            loss = model(**data).loss\n",
    "            # accululating gradients over steps\n",
    "            if gradient_accumulation_steps > 1:\n",
    "                loss = loss / gradient_accumulation_steps\n",
    "            loss.backward()\n",
    "            if step % gradient_accumulation_steps == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler.step()\n",
    "            \n",
    "            rm.add(loss.item())\n",
    "            mean_loss = rm.mean_all_rank()\n",
    "            if is_master(rank):\n",
    "                print(f'loss: {mean_loss}')\n",
    "                writer.add_scalar('Loss/train', mean_loss, global_step)      \n",
    "                # with open(os.path.join(args.output_dir, \"log.csv\"), \"a\") as f_log:\n",
    "                #     f_log.write(str(global_step)+\",\"+str(mean_loss)+\"\\n\")\n",
    "            \n",
    "            if(global_step%(args.save_checkpoint_steps*gradient_accumulation_steps)==0 and global_step>0):\n",
    "                dir_name = 'model_'+str(global_step)\n",
    "                checkpoint_dir = os.path.join(args.output_dir, dir_name)\n",
    "                if not os.path.exists(checkpoint_dir):\n",
    "                    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "                print(\"saving checkpoint...\")\n",
    "                utils.save_model_checkpoint(model, checkpoint_dir, rank)\n",
    "                \n",
    "                \n",
    "            dist.barrier()\n",
    "            global_step += 1\n",
    "        \n",
    "    if is_master(rank):\n",
    "        writer.flush()\n",
    "        writer.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # parser.add_argument('--load_from', type=str, default='/data/local/IndexQuality/FinetuneLLM/Phi-3-medium')\n",
    "    parser.add_argument('--load_from', type=str, default='intfloat/e5-mistral-7b-instruct')\n",
    "    parser.add_argument('--model_path', type=str, default='/cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/Mixtral_New_ym_e5/current_best_1200/pytorch_model.bin')\n",
    "    parser.add_argument('--model', type=str, default='mixtral')\n",
    "    parser.add_argument('--num_workers', type=int, default=4)\n",
    "    parser.add_argument('--warmup', type=float, default=0.1)\n",
    "    parser.add_argument('--training_data_from', type=str, default=\"/cosmos/local/IndexQuality/FinetuneLLM/TrainingData/O1_A3_crowd_llm_training_data_no_eval_dataset_overlap.tsv\")\n",
    "    parser.add_argument('--columns', '-c', nargs='+', required=True, help=\"Names of columns to read\")\n",
    "    parser.add_argument('--batch_size', type=int, default=10) # 8->10 \n",
    "    parser.add_argument('--gradient_accumulation_steps', type=int, default=2)\n",
    "    parser.add_argument('--num_epochs', type=int, default=2)\n",
    "    parser.add_argument('--max_seq_length', type=int, default=1024)\n",
    "    # parser.add_argument('--eval_steps', type=int, default=200)  # 平常隔200评估一次，如果效果比之前好，直接保存；否则按save_checkpoint_steps保存\n",
    "    parser.add_argument('--save_checkpoint_steps', type=int, default=100)\n",
    "    parser.add_argument('--lr', type=float, default=1e-5)\n",
    "    parser.add_argument('--gpu_counts', type=float, default=16) # warmup=0.1\n",
    "    parser.add_argument('--output_dir', type=str, required=True)\n",
    "    parser.add_argument('--experiment_name', type=str, required=True)\n",
    "    parser.add_argument('--log_dir', type=str, required=True)\n",
    "    args = parser.parse_args()\n",
    "    train(args, *setup(args))\n",
    "    \n",
    "# singularity Command\n",
    "# pip install transformers[torch]==4.38.1 datasets scikit-learn dataclasses lightgbm matplotlib mlflow tensorboard && cd /cosmos/local/IndexQuality/ContentModels/DataAugmentation/data/CBSpam_v3/Code/FinetuneLLM-US/new/ && CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 torchrun --nnodes 1 --nproc_per_node 16 train_script_optimized_test.py --load_from /cosmos/local/IndexQuality/FinetuneLLM/Mixtral-8x7B-Instruct-v0.1/ --model mixtral --num_workers 4 --training_data_from /cosmos/local/IndexQuality/FinetuneLLM/TrainingData/O1_A3_crowd_llm_training_data_no_eval_dataset_overlap.tsv --batch_size 10 --gradient_accumulation_steps 2 --num_epochs 2 --save_checkpoint_steps 200 --lr 1e-5 --output_dir /cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/Mixtral_New_ym_bs_10_test --experiment_name MixtralTrain_ym --disable_tensorboard False --log_dir /cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/Mixtral_New_ym_bs_10_test/log\n",
    "# current running: Mixtral_again: https://ml.azure.com/runs/Mixtral_again?wsid=/subscriptions/7972af26-e54d-410e-a755-20e582a46de0/resourceGroups/singularity-webdata/providers/Microsoft.MachineLearningServices/workspaces/singularity-webdata-ws01-eastus2&flight=1ptraining&tid=72f988bf-86f1-41af-91ab-2d7cd011db47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export CUDA_VISIBLE_DEVICES=\"0,1,2\"\n",
    "!NCCL_DEBUG=WARN python -m torch.distributed.run  \\\n",
    "--master_port 29501 --nnodes 1 --nproc_per_node 3 train_script_prompt_e5_0.py \\\n",
    "--load_from intfloat/e5-mistral-7b-instruct \\\n",
    "--model mistral --num_workers 4 \\\n",
    "--training_data_from /cosmos/local/IndexQuality/FinetuneLLM/TrainingData/No_overlap_complete_O1_A3_crowd_training.tsv \\\n",
    "--columns Url UrlTitle UrlSnippet FullBody Label \\\n",
    "--batch_size 20 \\\n",
    "--gradient_accumulation_steps 1 \\\n",
    "--num_epochs 2 \\\n",
    "--max_seq_length 1024 \\\n",
    "--save_checkpoint_steps 100 \\\n",
    "--lr 1e-5 \\\n",
    "--gpu_counts 3 \\\n",
    "--output_dir /cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/E5_prompt_v0 \\\n",
    "--experiment_name mistral_e5_v7 \\\n",
    "--log_dir /cosmos/local/IndexQuality/FinetuneLLM/FullTrainTest/E5_prompt_v0/logs\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ptca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
